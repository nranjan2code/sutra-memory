services:
  # ============================================================================
  # STORAGE LAYER
  # ============================================================================
  
  # ============================================================================
  # REVERSE PROXY & LOAD BALANCER
  # ============================================================================

  # Nginx Reverse Proxy (Single Entry Point for All External Traffic)
  nginx-proxy:
    build:
      context: .
      dockerfile: ./nginx/Dockerfile
    image: sutra-works-nginx-proxy:${SUTRA_VERSION:-latest}
    container_name: sutra-works-nginx-proxy
    ports:
      - "80:80"      # HTTP (redirects to HTTPS in production)
      - "443:443"    # HTTPS (production)
      - "8080:8080"  # Development (no SSL)
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - nginx-logs:/var/log/nginx
      # Mount SSL certificates for production (optional - uses self-signed by default)
      # - ${SUTRA_SSL_CERT_PATH}:/etc/nginx/ssl/cert.pem:ro
      # - ${SUTRA_SSL_KEY_PATH}:/etc/nginx/ssl/key.pem:ro
    environment:
      - NGINX_WORKER_PROCESSES=auto
      - NGINX_WORKER_CONNECTIONS=16384
    depends_on:
      - sutra-api
      - sutra-hybrid
      - sutra-client
      - sutra-control
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ============================================================================
  # STORAGE LAYER (Internal Only - No External Exposure)
  # ============================================================================

  # Main Storage Server (Core Sutra AI Knowledge Graph)
  storage-server:
    build:
      context: ../..
      dockerfile: ./packages/sutra-storage/Dockerfile
    image: sutra-works-storage-server:${SUTRA_VERSION:-latest}
    container_name: sutra-works-storage
    # SECURITY: Removed external port exposure - only accessible via internal network
    expose:
      - "50051"
    volumes:
      - storage-data:/data
    environment:
      - RUST_LOG=info
      - STORAGE_PATH=/data
      - STORAGE_HOST=0.0.0.0
      - STORAGE_PORT=50051
      - VECTOR_DIMENSION=${MATRYOSHKA_DIM:-768}  # Phase 0: Match Matryoshka dimension
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Sharded Storage Configuration (edition-dependent)
      - SUTRA_NUM_SHARDS=${SUTRA_NUM_SHARDS:-1}
      # Unified Learning Pipeline Configuration (edition-dependent)
      - SUTRA_EMBEDDING_SERVICE_URL=${SUTRA_EMBEDDING_URL:-http://sutra-works-embedding-single:8888}
      - SUTRA_EMBEDDING_TIMEOUT_SEC=30
      - SUTRA_MIN_ASSOCIATION_CONFIDENCE=0.5
      - SUTRA_MAX_ASSOCIATIONS_PER_CONCEPT=10
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/50051' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Grid Event Storage (Reserved for Grid Observability - Enterprise Only)
  grid-event-storage:
    build:
      context: ../..
      dockerfile: ./packages/sutra-storage/Dockerfile
    image: sutra-works-storage-server:${SUTRA_VERSION:-latest}
    container_name: sutra-works-grid-events
    # SECURITY: Internal only - no external exposure
    expose:
      - "50051"
    volumes:
      - grid-event-data:/data
    environment:
      - RUST_LOG=info
      - STORAGE_PATH=/data
      - STORAGE_HOST=0.0.0.0
      - STORAGE_PORT=50051
      - VECTOR_DIMENSION=768
      # Edition Configuration
      - SUTRA_EDITION=enterprise
      # Use HA embedding service (same as main storage)
      - SUTRA_EMBEDDING_SERVICE_URL=http://embedding-ha:8888
      - SUTRA_EMBEDDING_TIMEOUT_SEC=30
      - SUTRA_MIN_ASSOCIATION_CONFIDENCE=0.5
      - SUTRA_MAX_ASSOCIATIONS_PER_CONCEPT=10
    depends_on:
      embedding-ha:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/50051' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    profiles:
      - enterprise

  # User Storage (User Management, Auth, Conversations - All Editions)
  user-storage-server:
    build:
      context: ../..
      dockerfile: ./packages/sutra-storage/Dockerfile
    image: sutra-works-storage-server:${SUTRA_VERSION:-latest}
    container_name: sutra-works-user-storage
    # SECURITY: Internal only - no external exposure
    expose:
      - "50051"
    volumes:
      - user-storage-data:/data
    environment:
      - RUST_LOG=info
      - STORAGE_PATH=/data
      - STORAGE_HOST=0.0.0.0
      - STORAGE_PORT=50051
      - VECTOR_DIMENSION=${MATRYOSHKA_DIM:-768}  # Match Phase 0 dimension
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Unified Learning Pipeline Configuration
      - SUTRA_EMBEDDING_SERVICE_URL=${SUTRA_EMBEDDING_URL:-http://sutra-works-embedding-single:8888}
      - SUTRA_EMBEDDING_TIMEOUT_SEC=30
      - SUTRA_MIN_ASSOCIATION_CONFIDENCE=0.5
      - SUTRA_MAX_ASSOCIATIONS_PER_CONCEPT=10
      - SUTRA_SEMANTIC_ANALYSIS=false  # Disable for user storage (auth data only)
    depends_on:
      storage-server:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/50051' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
  
  # Phase 1: Dedicated Sutra Storage Cache Shard (L2 cache for embeddings)
  storage-cache-shard:
    build:
      context: ../..
      dockerfile: ./packages/sutra-storage/Dockerfile
    image: sutra-works-storage-server:${SUTRA_VERSION:-latest}
    container_name: sutra-works-storage-cache
    # SECURITY: Internal only
    expose:
      - "50052"  # Different port from main storage
    volumes:
      - storage-cache-data:/data
    environment:
      - RUST_LOG=info
      - STORAGE_PATH=/data
      - STORAGE_HOST=0.0.0.0
      - STORAGE_PORT=50052
      - VECTOR_DIMENSION=${MATRYOSHKA_DIM:-256}  # Match Phase 0
      # Cache-Optimized Settings
      - SUTRA_ROLE=cache_shard
      - CACHE_MODE=true
      - MAX_CONCEPTS=100000  # Large capacity for cache
      - EVICTION_POLICY=lru
      - DEFAULT_TTL_SECONDS=86400  # 24 hours
      - MMAP_ENABLED=true  # Memory-mapped vectors (fast)
      - WAL_SYNC_INTERVAL_MS=5000  # Less aggressive (cache can rebuild)
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/50052' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 2G  # Smaller than main storage
        reservations:
          memory: 1G
    profiles:
      - scaling  # Deploy with scaling profile

  # ============================================================================
  # GRID INFRASTRUCTURE
  # ============================================================================
  
  # Grid Master (Orchestration & Coordination - Enterprise Only)
  grid-master:
    build:
      context: ../..
      dockerfile: ./packages/sutra-grid-master/Dockerfile
    image: sutra-works-grid-master:${SUTRA_VERSION:-latest}
    container_name: sutra-works-grid-master
    # SECURITY: Internal only - no external exposure
    expose:
      - "7001"  # HTTP binary distribution
      - "7002"  # TCP agent connections
    environment:
      - RUST_LOG=info
      - GRID_MASTER_HOST=0.0.0.0
      - GRID_MASTER_TCP_PORT=7002
      - GRID_MASTER_HTTP_PORT=7001
      - EVENT_STORAGE=grid-event-storage:50051
      # Edition enforcement
      - SUTRA_EDITION=enterprise
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
    depends_on:
      - grid-event-storage
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "7002"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    profiles:
      - enterprise

  # Grid Agent 1 (Node Management - Enterprise Only)
  grid-agent-1:
    build:
      context: ../..
      dockerfile: ./packages/sutra-grid-agent/Dockerfile
    image: sutra-works-grid-agent:${SUTRA_VERSION:-latest}
    container_name: sutra-works-grid-agent-1
    # SECURITY: Internal only - no external exposure
    expose:
      - "8001"
    volumes:
      - agent1-data:/storage-nodes
    environment:
      - RUST_LOG=info
      - GRID_AGENT_HOST=0.0.0.0
      - GRID_AGENT_PORT=8001
      - GRID_MASTER_ADDRESS=grid-master:7002
      - EVENT_STORAGE=grid-event-storage:50051
      - AGENT_ID=agent-001
      - MAX_STORAGE_NODES=5
      # Edition enforcement
      - SUTRA_EDITION=enterprise
    depends_on:
      - grid-master
      - grid-event-storage
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "grid-master", "7002"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s
    profiles:
      - enterprise

  # Grid Agent 2 (Additional Node Management - Enterprise Only)
  grid-agent-2:
    build:
      context: ../..
      dockerfile: ./packages/sutra-grid-agent/Dockerfile
    image: sutra-works-grid-agent:${SUTRA_VERSION:-latest}
    container_name: sutra-works-grid-agent-2
    # SECURITY: Internal only - no external exposure
    expose:
      - "8001"
    volumes:
      - agent2-data:/storage-nodes
    environment:
      - RUST_LOG=info
      - GRID_AGENT_HOST=0.0.0.0
      - GRID_AGENT_PORT=8001
      - GRID_MASTER_ADDRESS=grid-master:7002
      - EVENT_STORAGE=grid-event-storage:50051
      - AGENT_ID=agent-002
      - MAX_STORAGE_NODES=5
      # Edition enforcement
      - SUTRA_EDITION=enterprise
    depends_on:
      - grid-master
      - grid-event-storage
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "grid-master", "7002"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s
    profiles:
      - enterprise

  # ============================================================================
  # API LAYER
  # ============================================================================
  
  # Sutra API (Primary REST API)
  sutra-api:
    build:
      context: ../..
      dockerfile: ./packages/sutra-api/Dockerfile
    image: sutra-works-api:${SUTRA_VERSION:-latest}
    container_name: sutra-works-api
    # SECURITY: Access via nginx proxy only
    expose:
      - "8000"
    environment:
      - PYTHONUNBUFFERED=1
      - SUTRA_STORAGE_SERVER=storage-server:50051
      - SUTRA_USER_STORAGE_SERVER=user-storage-server:50051
      # JWT Configuration - REQUIRED: Set SUTRA_JWT_SECRET_KEY in .env file
      - SUTRA_JWT_SECRET_KEY=${SUTRA_JWT_SECRET_KEY}
      - SUTRA_JWT_ALGORITHM=HS256
      - SUTRA_JWT_EXPIRATION_HOURS=24
      - SUTRA_JWT_REFRESH_EXPIRATION_DAYS=7
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      - SUTRA_LICENSE_SECRET=${SUTRA_LICENSE_SECRET}
      # Rate limits DISABLED for maximum stress testing
      - SUTRA_RATE_LIMIT_LEARN=1000000
      - SUTRA_RATE_LIMIT_REASON=1000000
    depends_on:
      - storage-server
      - user-storage-server
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  # ============================================================================
  # ML FOUNDATION SERVICES
  # ============================================================================
  
  # ML-Base Service (Centralized ML Inference Platform)
  # ============================================================================
  # PHASE 0+1+2 SCALING: ML-BASE SERVICES WITH LOAD BALANCING
  # ============================================================================
  
  # Phase 2: HAProxy Load Balancer for ML-Base Replicas (3× horizontal scaling)
  ml-base-lb:
    build:
      context: ../..
      dockerfile: ./haproxy/Dockerfile
    image: sutra-works-haproxy:${SUTRA_VERSION:-latest}
    container_name: sutra-works-ml-base-lb
    # SECURITY: Internal only
    expose:
      - "8887"  # Frontend
      - "9999"  # Stats
    networks:
      - sutra-network
    depends_on:
      ml-base-1:
        condition: service_healthy
      ml-base-2:
        condition: service_healthy
      ml-base-3:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      interval: 30s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M
    profiles:
      - scaling  # Deploy with: SUTRA_EDITION=simple docker-compose --profile scaling up
  
  # ML-Base Replica 1 (Phase 0: Matryoshka + Phase 2: Horizontal scaling)
  ml-base-1:
    build:
      context: ../../packages/sutra-ml-base-service
      dockerfile: Dockerfile
    image: sutra-works-ml-base-service:${SUTRA_VERSION:-latest}
    container_name: sutra-works-ml-base-1
    expose:
      - "8887"
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      - INSTANCE_ID=ml-base-1
      # Phase 0: Matryoshka configuration (3× improvement)
      - MATRYOSHKA_DIM=${MATRYOSHKA_DIM:-256}  # 256/512/768
      - VECTOR_DIMENSION=${MATRYOSHKA_DIM:-256}
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Model Configuration
      - ML_BASE_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - ML_BASE_NLG_MODEL=google/gemma-2-2b-it
      - ML_BASE_CACHE_SIZE=10000
      - ML_BASE_MODEL_CACHE_DIR=/models/cache
      # Performance Configuration
      - ML_BASE_MAX_BATCH_SIZE=256
      - ML_BASE_BATCH_TIMEOUT_MS=10
      - ML_BASE_MODEL_UNLOAD_TIMEOUT=600
      # Monitoring
      - ML_BASE_METRICS_ENABLED=true
      - ML_BASE_LOG_LEVEL=INFO
    volumes:
      - ml-models-cache:/models/cache
      - /var/log/sutra:/var/log/sutra
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8887/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G
    profiles:
      - scaling
  
  # ML-Base Replica 2
  ml-base-2:
    build:
      context: ../../packages/sutra-ml-base-service
      dockerfile: Dockerfile
    image: sutra-works-ml-base-service:${SUTRA_VERSION:-latest}
    container_name: sutra-works-ml-base-2
    expose:
      - "8887"
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      - INSTANCE_ID=ml-base-2
      - MATRYOSHKA_DIM=${MATRYOSHKA_DIM:-256}
      - VECTOR_DIMENSION=${MATRYOSHKA_DIM:-256}
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      - ML_BASE_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - ML_BASE_NLG_MODEL=google/gemma-2-2b-it
      - ML_BASE_CACHE_SIZE=10000
      - ML_BASE_MODEL_CACHE_DIR=/models/cache
      - ML_BASE_MAX_BATCH_SIZE=256
      - ML_BASE_BATCH_TIMEOUT_MS=10
      - ML_BASE_MODEL_UNLOAD_TIMEOUT=600
      - ML_BASE_METRICS_ENABLED=true
      - ML_BASE_LOG_LEVEL=INFO
    volumes:
      - ml-models-cache:/models/cache
      - /var/log/sutra:/var/log/sutra
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8887/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G
    profiles:
      - scaling
  
  # ML-Base Replica 3
  ml-base-3:
    build:
      context: ../../packages/sutra-ml-base-service
      dockerfile: Dockerfile
    image: sutra-works-ml-base-service:${SUTRA_VERSION:-latest}
    container_name: sutra-works-ml-base-3
    expose:
      - "8887"
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      - INSTANCE_ID=ml-base-3
      - MATRYOSHKA_DIM=${MATRYOSHKA_DIM:-256}
      - VECTOR_DIMENSION=${MATRYOSHKA_DIM:-256}
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      - ML_BASE_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - ML_BASE_NLG_MODEL=google/gemma-2-2b-it
      - ML_BASE_CACHE_SIZE=10000
      - ML_BASE_MODEL_CACHE_DIR=/models/cache
      - ML_BASE_MAX_BATCH_SIZE=256
      - ML_BASE_BATCH_TIMEOUT_MS=10
      - ML_BASE_MODEL_UNLOAD_TIMEOUT=600
      - ML_BASE_METRICS_ENABLED=true
      - ML_BASE_LOG_LEVEL=INFO
    volumes:
      - ml-models-cache:/models/cache
      - /var/log/sutra:/var/log/sutra
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8887/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G
    profiles:
      - scaling
  
  # Legacy ML-Base Service (Simple/Community - No scaling)
  ml-base-service:
    build:
      context: ../../packages/sutra-ml-base-service
      dockerfile: Dockerfile
    image: sutra-works-ml-base-service:${SUTRA_VERSION:-latest}
    container_name: sutra-works-ml-base
    # SECURITY: Internal only - ML inference backend
    expose:
      - "8887"
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      # Phase 0: Matryoshka configuration
      - MATRYOSHKA_DIM=${MATRYOSHKA_DIM:-768}  # Default to full unless overridden
      - VECTOR_DIMENSION=${MATRYOSHKA_DIM:-768}
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Model Configuration
      - ML_BASE_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - ML_BASE_NLG_MODEL=google/gemma-2-2b-it
      - ML_BASE_CACHE_SIZE=10000
      - ML_BASE_MODEL_CACHE_DIR=/models/cache
      # Performance Configuration
      - ML_BASE_MAX_BATCH_SIZE=256
      - ML_BASE_BATCH_TIMEOUT_MS=10
      - ML_BASE_MODEL_UNLOAD_TIMEOUT=600
      # Monitoring
      - ML_BASE_METRICS_ENABLED=true
      - ML_BASE_LOG_LEVEL=INFO
    volumes:
      - ml-models-cache:/models/cache
      - /var/log/sutra:/var/log/sutra
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8887/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G
    profiles:
      - simple
      - community

  # ============================================================================
  # ADVANCED EMBEDDING SERVICES (External - 4x Performance)
  # ============================================================================
  
  # Single Advanced Embedding Service (Simple & Community Editions)
  # Uses external ghcr.io/nranjan2code/sutra-embedder:v1.0.1 (4x faster Rust implementation)
  sutra-works-embedding-single:
    image: ghcr.io/nranjan2code/sutra-embedder:v1.0.1
    container_name: sutra-works-embedding-single
    # SECURITY: Internal only - embedding service backend
    expose:
      - "8888"
    environment:
      - PYTHONUNBUFFERED=1
      - EMBEDDING_PORT=8888
      - EMBEDDING_HOST=0.0.0.0
      # Configuration
      - VECTOR_DIMENSION=${MATRYOSHKA_DIM:-768}
      - BATCH_SIZE=32
      - MODEL_NAME=nomic-ai/nomic-embed-text-v1.5
      # Instance Configuration
      - INSTANCE_ID=embedding-single
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    profiles:
      - simple
      - community
  
  # ============================================================================
  # ADVANCED EMBEDDING SERVICE HA (Enterprise Edition Only)
  # ============================================================================
  
  # Advanced Embedding Service Replica 1
  embedding-1:
    image: ghcr.io/nranjan2code/sutra-embedder:v1.0.1
    container_name: sutra-works-embedding-1
    environment:
      - RUST_LOG=info
      - EMBEDDING_PORT=8888
      - EMBEDDING_HOST=0.0.0.0
      - MODEL_PATH=/models/nomic-embed-text-v1.5
      - VECTOR_DIMENSION=768
      - CACHE_SIZE=50000
      - BATCH_SIZE=256
      - INSTANCE_ID=embedding-1
      # Advanced Performance Features
      - SIMD_ACCELERATION=true
      - GPU_ACCELERATION=auto
      - QUANTIZATION=int8
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    profiles:
      - enterprise
  
  # Advanced Embedding Service Replica 2
  embedding-2:
    image: ghcr.io/nranjan2code/sutra-embedder:v1.0.1
    container_name: sutra-works-embedding-2
    environment:
      - RUST_LOG=info
      - EMBEDDING_PORT=8888
      - EMBEDDING_HOST=0.0.0.0
      - MODEL_PATH=/models/nomic-embed-text-v1.5
      - VECTOR_DIMENSION=768
      - CACHE_SIZE=50000
      - BATCH_SIZE=256
      - INSTANCE_ID=embedding-2
      # Advanced Performance Features
      - SIMD_ACCELERATION=true
      - GPU_ACCELERATION=auto
      - QUANTIZATION=int8
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    profiles:
      - enterprise
  
  # Advanced Embedding Service Replica 3
  embedding-3:
    image: ghcr.io/nranjan2code/sutra-embedder:v1.0.1
    container_name: sutra-works-embedding-3
    environment:
      - RUST_LOG=info
      - EMBEDDING_PORT=8888
      - EMBEDDING_HOST=0.0.0.0
      - MODEL_PATH=/models/nomic-embed-text-v1.5
      - VECTOR_DIMENSION=768
      - CACHE_SIZE=50000
      - BATCH_SIZE=256
      - INSTANCE_ID=embedding-3
      # Advanced Performance Features
      - SIMD_ACCELERATION=true
      - GPU_ACCELERATION=auto
      - QUANTIZATION=int8
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    profiles:
      - enterprise
  
  # NOTE: HAProxy load balancer temporarily removed for external service integration
  # Will be re-added once external services are proven to work
  # embedding-ha:
  #   Configuration moved to future phase
  

  # ============================================================================
  # ADVANCED NLG SERVICES (External - Enterprise AI Framework)
  # ============================================================================
  
  # Advanced NLG Service (Simple/Community Edition)
  # Uses external ghcr.io/nranjan2code/sutraworks-model:v1.0.0 (Enterprise AI with RWKV/Mamba)
  sutra-works-nlg-single:
    image: ghcr.io/nranjan2code/sutraworks-model:v1.0.0
    container_name: sutra-works-nlg-single
    # SECURITY: Internal only - advanced NLG service backend
    expose:
      - "8003"
    environment:
      - RUST_LOG=info
      - NLG_PORT=8003
      - NLG_HOST=0.0.0.0
      # Advanced AI Configuration
      - MODEL_TYPE=rwkv  # Default to RWKV model
      - MAX_TOKENS=512
      - TEMPERATURE=0.7
      - WARMUP=true
      # Enterprise Features
      - REQUEST_TRACKING=true
      - METRICS_ENABLED=true
      - PERFORMANCE_MONITORING=true
      # Instance Configuration
      - INSTANCE_ID=nlg-single
    networks:
      - sutra-network
    restart: "on-failure:3"  # NLG is optional - stop after 3 failed attempts
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s  # Longer startup for model loading
    deploy:
      resources:
        limits:
          memory: 4G  # Enterprise AI requires more memory
        reservations:
          memory: 2G
    profiles:
      - simple
      - community
  
  # ============================================================================
  # ADVANCED NLG SERVICE HA (Enterprise Edition Only)
  # ============================================================================
  
  # Advanced NLG Service Replica 1
  nlg-1:
    image: ghcr.io/nranjan2code/sutraworks-model:v1.0.0
    container_name: sutra-works-nlg-1
    environment:
      - RUST_LOG=info
      - NLG_PORT=8003
      - NLG_HOST=0.0.0.0
      # Advanced AI Configuration
      - MODEL_TYPE=rwkv  # RWKV model for replica 1
      - MAX_TOKENS=512
      - TEMPERATURE=0.7
      - WARMUP=true
      - INSTANCE_ID=nlg-1
      # Enterprise Features
      - REQUEST_TRACKING=true
      - METRICS_ENABLED=true
      - PERFORMANCE_MONITORING=true
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    profiles:
      - enterprise
  
  # Advanced NLG Service Replica 2
  nlg-2:
    image: ghcr.io/nranjan2code/sutraworks-model:v1.0.0
    container_name: sutra-works-nlg-2
    environment:
      - RUST_LOG=info
      - NLG_PORT=8003
      - NLG_HOST=0.0.0.0
      # Advanced AI Configuration
      - MODEL_TYPE=mamba  # Mamba model for replica 2 (diversity)
      - MAX_TOKENS=512
      - TEMPERATURE=0.7
      - WARMUP=true
      - INSTANCE_ID=nlg-2
      # Enterprise Features
      - REQUEST_TRACKING=true
      - METRICS_ENABLED=true
      - PERFORMANCE_MONITORING=true
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    profiles:
      - enterprise
  
  # Advanced NLG Service Replica 3
  nlg-3:
    image: ghcr.io/nranjan2code/sutraworks-model:v1.0.0
    container_name: sutra-works-nlg-3
    environment:
      - RUST_LOG=info
      - NLG_PORT=8003
      - NLG_HOST=0.0.0.0
      # Advanced AI Configuration
      - MODEL_TYPE=auto  # Auto-selection for replica 3
      - MAX_TOKENS=512
      - TEMPERATURE=0.7
      - WARMUP=true
      - INSTANCE_ID=nlg-3
      # Enterprise Features
      - REQUEST_TRACKING=true
      - METRICS_ENABLED=true
      - PERFORMANCE_MONITORING=true
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    profiles:
      - enterprise
  
  # NOTE: NLG HAProxy load balancer temporarily removed for external service integration
  # Will be re-added once external services are proven to work
  # nlg-ha:
  #   Configuration moved to future phase

  # Sutra Hybrid (Semantic Embeddings API + NLG)
  sutra-hybrid:
    build:
      context: ../..
      dockerfile: ./packages/sutra-hybrid/Dockerfile
    image: sutra-works-hybrid:${SUTRA_VERSION:-latest}
    container_name: sutra-works-hybrid
    # SECURITY: Access via nginx proxy only
    expose:
      - "8000"
    environment:
      - PYTHONUNBUFFERED=1
      - SUTRA_STORAGE_SERVER=storage-server:50051
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      - SUTRA_LICENSE_SECRET=${SUTRA_LICENSE_SECRET}
      # Semantic Embeddings
      - SUTRA_USE_SEMANTIC_EMBEDDINGS=true
      - SUTRA_EMBEDDING_PROVIDER=service
      - SUTRA_EMBEDDING_SERVICE_URL=${SUTRA_EMBEDDING_URL:-http://sutra-works-embedding-single:8888}
      - SUTRA_EMBEDDING_MODEL=nomic-embed-text
      - SUTRA_VECTOR_DIMENSION=${MATRYOSHKA_DIM:-768}  # Phase 0: Use Matryoshka dimension
      # NLG Configuration (always enabled)
      - SUTRA_NLG_ENABLED=true
      - SUTRA_NLG_MODE=${SUTRA_NLG_MODE:-hybrid}
      - SUTRA_NLG_SERVICE_URL=${SUTRA_NLG_URL:-http://sutra-works-nlg-single:8003}
    depends_on:
      - storage-server
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/ping"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  # ============================================================================
  # WEB INTERFACES
  # ============================================================================
  
  # Sutra Control Center (Grid Management + System Monitoring)
  sutra-control:
    build:
      context: ../..
      dockerfile: ./packages/sutra-control/Dockerfile
    image: sutra-works-control:${SUTRA_IMAGE_TAG:-latest}
    container_name: sutra-works-control
    # SECURITY: Access via nginx proxy only
    expose:
      - "9000"
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - ENVIRONMENT=production
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Service URLs
      - SUTRA_STORAGE_SERVER=storage-server:50051
      - SUTRA_GRID_MASTER=${SUTRA_GRID_MASTER_URL:-}
      - PORT=9000
    depends_on:
      - storage-server
      - sutra-api
      - sutra-hybrid
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 15s


  # Sutra Client (Interactive AI Interface)
  # Sutra Client (Conversation-First UI)
  sutra-client:
    build:
      context: ../../packages/sutra-client
      dockerfile: Dockerfile
    image: sutra-works-client:${SUTRA_VERSION:-latest}
    container_name: sutra-works-client
    # SECURITY: Access via nginx proxy only
    expose:
      - "8080"
    environment:
      # Edition Configuration (for UI display)
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      # Service URLs (nginx proxies internally, not exposed to client)
      - SUTRA_API_URL=http://sutra-api:8000
      - SUTRA_HYBRID_URL=http://sutra-hybrid:8001
    depends_on:
      sutra-api:
        condition: service_healthy
      user-storage-server:
        condition: service_healthy
      sutra-hybrid:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  # Sutra Explorer Backend (Storage Visualization API)
  sutra-explorer-backend:
    build:
      context: ../../packages/sutra-explorer
      dockerfile: ./backend/Dockerfile
    image: sutra-works-explorer-backend:${SUTRA_VERSION:-latest}
    container_name: sutra-works-explorer-backend
    # SECURITY: Access via nginx proxy only
    expose:
      - "8100"
    environment:
      # API Configuration
      - API_HOST=0.0.0.0
      - API_PORT=8100
      - LOG_LEVEL=INFO
      # Storage Client Connections (via TCP binary protocol)
      - SUTRA_USER_STORAGE=user-storage-server:50051
      - SUTRA_DOMAIN_STORAGE=storage-server:50051
      # CORS Configuration
      - CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:8080
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
    depends_on:
      storage-server:
        condition: service_healthy
      user-storage-server:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8100/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    profiles:
      - explorer

  # Sutra Explorer Frontend (React + D3.js Visualization)
  sutra-explorer-frontend:
    build:
      context: ../../packages/sutra-explorer
      dockerfile: ./frontend/Dockerfile
      args:
        - VITE_API_URL=http://localhost:8100
    image: sutra-works-explorer-frontend:${SUTRA_VERSION:-latest}
    container_name: sutra-works-explorer-frontend
    # SECURITY: Access via nginx proxy only
    expose:
      - "3000"
    environment:
      - VITE_API_URL=http://sutra-explorer-backend:8100
    depends_on:
      - sutra-explorer-backend
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    profiles:
      - explorer

  # ============================================================================
  # BULK INGESTION LAYER (Optional - can be disabled)
  # ============================================================================
  
  # Primary Bulk Ingester Service
  sutra-bulk-ingester:
    build:
      context: ../..
      dockerfile: ./packages/sutra-bulk-ingester/Dockerfile
    image: sutra-works-bulk-ingester:${SUTRA_VERSION:-latest}
    container_name: sutra-works-bulk-ingester
    # SECURITY: Access via nginx proxy only (optional service)
    expose:
      - "8005"
    volumes:
      - ./datasets:/datasets:ro  # Read-only dataset access
      - ingestion-jobs:/jobs     # Job state persistence
    environment:
      - RUST_LOG=info
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Service Configuration
      - SUTRA_STORAGE_SERVER=storage-server:50051
      - SUTRA_EMBEDDING_SERVICE_URL=${SUTRA_EMBEDDING_URL:-http://sutra-works-embedding-single:8888}
      - SUTRA_GRID_MASTER=${SUTRA_GRID_MASTER_URL:-}
      # Worker configuration (set by feature flags based on edition)
      - INGESTER_WORKERS=${SUTRA_INGEST_WORKERS:-2}
      - INGESTER_BATCH_SIZE=100
      - INGESTER_MEMORY_LIMIT=2048
    depends_on:
      storage-server:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 20s

# ============================================================================
# NETWORKING & STORAGE
# ============================================================================

volumes:
  # Docker managed volumes (easier for deployment)
  storage-data:
    driver: local
  grid-event-data:
    driver: local
  user-storage-data:
    driver: local
  
  # Phase 1: Cache storage shard data volume
  storage-cache-data:
    driver: local
  agent1-data:
    driver: local
  agent2-data:
    driver: local
  # New volume for bulk ingestion (only created when bulk-ingester profile used)
  ingestion-jobs:
    driver: local
  # ML model cache volume
  ml-models-cache:
    driver: local
  # Nginx logs volume
  nginx-logs:
    driver: local

networks:
  sutra-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
