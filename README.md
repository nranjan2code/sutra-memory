# Sutra AI

An explainable AI system that learns in real-time without retraining. Every decision includes reasoning paths showing how it arrived at an answer.

**ğŸš€ NEW: Production-Ready with Self-Observability, Quality Gates, and Streaming Responses**

Version 2.0 includes enterprise-grade production features:
- **Self-Observability**: Query your system's behavior using natural language ("Show me slow queries today")
- **Quality Gates**: Automatic confidence calibration - knows when to say "I don't know"
- **Streaming Responses**: Progressive answer refinement (10x faster perceived performance)
- **Event System**: Zero external dependencies - monitors itself using its own reasoning

**ğŸ“– [Complete Production Guide](docs/PRODUCTION_GUIDE.md)** | [Architecture](WARP.md) | [Deployment](DEPLOYMENT.md)

## Why This Exists

Current AI systems (LLMs) are black boxes:
- You can't see how they make decisions
- You can't verify their reasoning
- You can't update them without complete retraining
- You can't use them in regulated industries that require explainability

We're building an alternative that:
- Shows its reasoning for every answer
- Learns incrementally from new information
- Provides audit trails for compliance
- Works without requiring GPUs or massive compute

## What It Does

Sutra AI combines graph-based reasoning with semantic embeddings:

1. **Graph reasoning**: Concepts connected by typed relationships (semantic, causal, temporal, hierarchical, compositional)
2. **Semantic embeddings**: Optional similarity matching to enhance reasoning
3. **Multi-strategy comparison**: Compare different reasoning approaches and see agreement scores
4. **Real-time learning**: Learn from new information without retraining
5. **Full audit trails**: Every decision logged with timestamps, confidence scores, and reasoning paths

### Production Features (Version 2.0) âœ¨

**Self-Observability:**
- Events stored as concepts in knowledge graph
- Query operational data with natural language
- 30+ event types (query, learning, storage, system)
- Zero external monitoring dependencies

**Quality Gates:**
- Confidence calibration based on consensus and path diversity
- Automatic "I don't know" for uncertain answers
- Three presets: STRICT, MODERATE, LENIENT
- Explainable uncertainty quantification

**Streaming Responses:**
- Progressive answer refinement in 4 stages
- Server-Sent Events (SSE) protocol
- First response in 60ms (vs 500ms non-streaming)
- React/Vue/JavaScript client libraries included

**Natural Language Observability:**
- "Show me slow queries in the last hour"
- "What errors occurred today?"
- "How many low confidence queries?"
- Automatic insights generation

## What Works (Proven End-to-End)

âœ… **Learn new knowledge** - Add concepts and relationships  
âœ… **Query with reasoning paths** - Get answers with explanations  
âœ… **Save to disk** - Persist knowledge (concepts, associations, embeddings)  
âœ… **Reload from disk** - Restore complete state after restart  
âœ… **Multi-strategy reasoning** - Compare graph-only vs semantic-enhanced  
âœ… **Audit trails** - Full compliance tracking  
âœ… **REST API** - Production-ready HTTP interface  

Tested with 5 concepts, ~100ms query latency, full persistence verified.

## Architecture

**12-Service Production Ecosystem** with TCP binary protocol and containerized deployment. All services communicate via high-performance TCP with a secure React-based control center for monitoring.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Docker Network (sutra-network)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  sutra-control â”‚    â”‚  sutra-client  â”‚    â”‚ sutra-markdown-web â”‚  â”‚
â”‚  â”‚  (React + Fast â”‚    â”‚   (Streamlit   â”‚    â”‚   (Markdown API)   â”‚  â”‚
â”‚  â”‚   API Gateway) â”‚    â”‚    UI Client)  â”‚    â”‚    Port: 8002     â”‚  â”‚
â”‚  â”‚   Port: 9000   â”‚    â”‚   Port: 8080   â”‚    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚            â”‚                     â”‚            TCP                        â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€ Binary                   â”‚
â”‚                                     â”‚  Protocol                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   sutra-api     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â–¶â”‚       storage-server         â”‚  â”‚
â”‚  â”‚   (FastAPI)     â”‚              â”‚  â”‚    (Rust TCP Server)        â”‚  â”‚
â”‚  â”‚   Port: 8000    â”‚              â”‚  â”‚      Port: 50051            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                     â”‚            â”‚                       â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚            â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ sutra-hybrid  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”‚â—€â”´â”€â”€â”€â”€  sutra-ollama         â”‚  â”‚
â”‚  â”‚ (Embeddings + â”‚              â”‚  â”‚   (Local LLM Server)      â”‚  â”‚
â”‚  â”‚ Orchestration)â”‚              â”‚  â”‚      Port: 11434           â”‚  â”‚
â”‚  â”‚   Port: 8001   â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                           â”‚
â”‚                                     â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â”‚      sutra-bulk-ingester       â”‚            ğŸ”¥ NEW SERVICE        â”‚
â”‚  â”‚   (High-Performance Rust)      â”‚            Port: 8005           â”‚
â”‚  â”‚      Port: 8005               â”‚         (Production Ready)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   Sutra Grid (Distributed Layer)                    â”‚  â”‚
â”‚  â”‚  Grid Master (7001 HTTP, 7002 TCP) â—€â”€â”€TCPâ”€â”€â–¶ Grid Agents (8001)        â”‚  â”‚
â”‚  â”‚  Event Storage (50052 TCP)                                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Services
- **sutra-control**: React-based monitoring center with Grid management and bulk ingester UI
- **sutra-client**: Streamlit web interface for interactive queries  
- **sutra-api**: Primary REST API for AI operations
- **sutra-hybrid**: Semantic embeddings and orchestration
- **storage-server**: Rust TCP core storage engine (57K writes/sec)
- **sutra-bulk-ingester**: ğŸ”¥ **NEW** High-performance Rust bulk data ingestion (1K-10K articles/min)
- **sutra-markdown-web**: Document processing API
- **sutra-ollama**: Local LLM inference server

All services communicate via gRPC internally, with REST APIs for external access. The control center provides secure monitoring without exposing internal implementation details.

### Sutra Grid - Distributed Storage Orchestration

**NEW**: Production-ready distributed infrastructure with complete Docker deployment and web UI integration.

Sutra Grid manages storage nodes across multiple agents with:
- **Bidirectional gRPC**: Master â†” Agent communication (ports 7001 HTTP, 7002 gRPC)
- **Event-Driven Monitoring**: 17 structured events â†’ knowledge graph (port 50052)
- **Auto-Recovery**: Crashed nodes restart automatically (up to 3 times)
- **Production Features**: Retry logic, timeouts, health monitoring, graceful degradation
- **Web UI**: Complete Grid management via Sutra Control Center (port 9000)

**Key Innovation**: Grid monitors itself using Sutra's own platform - proving event-driven observability works without external LMT (Logs/Metrics/Telemetry) stack.

**Status**: Production-Ready âœ…  
- Master: 11 events emitted
- Agent: 2 node lifecycle events  
- Storage: Events as queryable concepts
- Docker: Complete containerized deployment
- Control Center: Grid management UI integrated
- Testing: End-to-end verified

**Architecture Details**: See [docs/grid/architecture/GRID_ARCHITECTURE.md](docs/grid/architecture/GRID_ARCHITECTURE.md) and [DEPLOYMENT.md](DEPLOYMENT.md) for complete documentation.

## ğŸš¨ CRITICAL PRODUCTION REQUIREMENTS

**âš ï¸ Before deployment, you MUST read:**
- [`PRODUCTION_CHECKLIST.md`](PRODUCTION_CHECKLIST.md) - Mandatory pre-deployment verification
- [`docs/EMBEDDING_TROUBLESHOOTING.md`](docs/EMBEDDING_TROUBLESHOOTING.md) - Critical fixes applied

**The system will NOT function without:**
1. Ollama service with `granite-embedding:30m` model
2. Proper TCP protocol implementation
3. Environment variables correctly configured

## Quick Start

**ğŸ“– [Full Production Guide](docs/PRODUCTION_GUIDE.md)** - Complete documentation with configuration, monitoring, API reference, best practices, and troubleshooting.

### 1. Deploy with Docker (Recommended)

**âš¡ Single command deployment:**

```bash
# First-time installation
./sutra-deploy.sh install

# Or start existing services
./sutra-deploy.sh up
```

**Access services:**
```bash
open http://localhost:9000    # Control Center (monitoring + Grid + bulk ingester)
open http://localhost:8080    # Interactive Client (queries)
open http://localhost:8000    # Primary API
open http://localhost:8001    # Hybrid API (Streaming + NLG)
```

**Manage deployment:**
```bash
./sutra-deploy.sh status      # Check system status
./sutra-deploy.sh logs        # View all logs
./sutra-deploy.sh maintenance # Interactive menu
./sutra-deploy.sh down        # Stop all services
```

**See [DEPLOYMENT.md](DEPLOYMENT.md) for complete documentation.**

### 2. Test End-to-End

```bash
# Run the end-to-end test
python test_direct_workflow.py
```

This tests: Learn â†’ Save â†’ Reload â†’ Query â†’ Multi-strategy â†’ Audit

### 3. Use the API

**Standard Query:**
```bash
# Query with quality gates
curl -X POST http://localhost:8001/sutra/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is AI?", "max_paths": 5}'
```

**Streaming Query:**
```bash
# Progressive answer refinement (SSE)
curl -X POST http://localhost:8001/sutra/stream/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is AI?", "enable_quality_gates": true}'
```

**Learn:**
```bash
# Add knowledge
curl -X POST http://localhost:8001/sutra/learn \
  -H "Content-Type: application/json" \
  -d '{"text": "Python is a programming language"}'
```

**Observability:**
```python
# Query system behavior with natural language
from sutra_core.observability_query import create_observability_interface
obs = create_observability_interface(engine.storage)
obs.query("Show me slow queries in the last hour")
```

## What We're Working Toward

**Short-term** (Working now):
- Graph-based reasoning with explainability âœ…
- Real-time learning without retraining âœ…
- Semantic similarity enhancement âœ…
- REST API âœ…

**Mid-term** (In progress):
- Replace LLM-style interfaces completely
- Streaming responses
- Multi-modal support (text + structured data)
- Distributed reasoning

**Long-term** (Research):
- Replace all black-box neural networks with explainable alternatives
- Provable correctness for critical decisions
- Zero-trust AI systems where every output is verifiable

## Performance Characteristics

Storage-server benchmarks (production):

- **Learning**: 0.02ms per concept (57,412/sec)
- **Query (read)**: <0.01ms via in-memory snapshot
- **Path finding**: ~1ms for 3-hop BFS (server-side)
- **Storage**: Single file, memory-mapped, lock-free writes
- **Vector search**: HNSW O(log N)

## Key Design Decisions

### Why Graph-Based?

Graphs are inherently explainable. You can trace every reasoning path. LLMs are not.

### Why Rust for Storage?

Python is great for logic but slow for I/O. Rust gives us:
- Zero-copy memory-mapped files
- Lock-free concurrency
- Predictable performance

### Why Optional Embeddings?

Pure graph reasoning is 100% explainable. Embeddings enhance it but add some opacity. We make it optional and always show contribution.

### Why REST API as Sole Interface?

Clean separation. Internal implementation can change without breaking users.

## Project Structure

```
sutra-models/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ sutra-core/          # Graph reasoning engine
â”‚   â”œâ”€â”€ sutra-storage/        # Rust storage backend  
â”‚   â”œâ”€â”€ sutra-hybrid/         # Semantic embeddings
â”‚   â””â”€â”€ sutra-api/            # REST API (FastAPI)
â”œâ”€â”€ test_direct_workflow.py   # End-to-end test
â”œâ”€â”€ test_api_workflow.py      # API integration test
â”œâ”€â”€ QUICK_START.md            # How to run and test
â””â”€â”€ README.md                 # This file
```

## Testing

```bash
# Test core package
make test-core

# Test end-to-end workflow (no API)
python test_direct_workflow.py

# Test API workflow (requires API server running)
python test_api_workflow.py

# Format code
make format

# Lint
make lint
```

## Configuration

Via environment variables or config files:

```bash
# Storage location
export SUTRA_STORAGE_PATH="./knowledge"

# Enable semantic embeddings
export SUTRA_USE_SEMANTIC_EMBEDDINGS="true"

# API settings
export SUTRA_API_PORT="8000"

# Rate limits
export SUTRA_RATE_LIMIT_LEARN="30"
export SUTRA_RATE_LIMIT_REASON="60"
```

## Dependencies

**Core**:
- Python 3.8+
- numpy
- sutra-storage (Rust, compiled to Python extension)

**Optional**:
- sentence-transformers (for semantic embeddings)
- spaCy (for enhanced NLP)
- FastAPI + uvicorn (for API server)

## What This Is Not

- **Not an LLM replacement yet** - We're working toward it, but not there yet
- **Not trained on massive datasets** - Learns from what you give it
- **Not a general knowledge base** - Specialized for your domain
- **Not "AI magic"** - Deterministic reasoning with explainable paths

## Current Capabilities

**Production-Ready Features:**
- âœ… 5-6 hop reasoning depth (configurable)
- âœ… Natural language generation (grounded, template-driven NLG)
- âœ… Natural language input (intent classification + NER)
- âœ… Quality gates with confidence calibration
- âœ… Streaming responses (SSE protocol)
- âœ… Self-observability with natural language queries
- âœ… 57K writes/sec storage, <0.01ms reads
- âœ… Zero data loss (Write-Ahead Log)

**Design Constraints (Not Limitations):**
1. **Specialized for regulated industries** - Optimized for compliance/audit use cases where explainability is mandatory
2. **Learns from your data** - Not pre-trained on massive datasets (by design)
3. **English-centric NLP** - Components optimized for English (can be extended)
4. **No common sense by default** - Explicit knowledge only (prevents hallucination)
5. **Transparent reasoning** - Graph-based, not black-box neural nets

## Contributing

We welcome contributions that align with the mission of explainable, accountable AI.

Before contributing:
1. Read the architecture docs in WARP.md
2. Run tests to verify your changes
3. Follow the existing code style (black + isort)
4. Add tests for new features

## Research Foundation

Built on published research:

- **Adaptive Focus Learning**: "LLM-Oriented Token-Adaptive Knowledge Distillation" (Oct 2024)
- **Multi-Path Plan Aggregation (MPPA)**: Consensus-based reasoning
- **Graph-based reasoning**: Decades of knowledge representation research

No proprietary "secret sauce" - all techniques are from published work.

## License

MIT License - see LICENSE file

## Contact

This is an active research project. We're figuring things out as we go.

Issues and pull requests welcome.

---

**Status**: Production-ready for internal use. API tested end-to-end. Full persistence verified.  
**Version**: 2.0.0  
**Last tested**: 2025-10-16
