# Sutra AI Production Guide

**Complete guide to production-ready Sutra AI with all enhancements.**

Version: 2.0  
Last Updated: 2025-10-19

---

## Table of Contents

1. [Quick Start](#quick-start)
2. [Architecture Overview](#architecture-overview)
3. [Production Features](#production-features)
4. [Deployment](#deployment)
5. [Configuration](#configuration)
6. [Monitoring & Observability](#monitoring--observability)
7. [API Reference](#api-reference)
8. [Best Practices](#best-practices)
9. [Troubleshooting](#troubleshooting)
10. [Performance Tuning](#performance-tuning)

---

## Quick Start

### One-Command Deployment

```bash
# Deploy full production stack (11 services)
./sutra-deploy.sh up

# Check status
./sutra-deploy.sh status

# View logs
./sutra-deploy.sh logs
```

**Services Available:**
- **Control Center**: http://localhost:9000 (React UI)
- **API**: http://localhost:8000 (REST)
- **Hybrid API**: http://localhost:8001 (Streaming + NLG)
- **Client**: http://localhost:8080 (Streamlit)
- **Ollama**: http://localhost:11434 (Embeddings)

### Simple Python Usage

```python
from sutra_core import ReasoningEngine

# Initialize (events + quality gates auto-enabled)
engine = ReasoningEngine()

# Learn knowledge
engine.learn("Machine learning is a subset of AI")

# Query with automatic quality checking
result = engine.ask("What is machine learning?")
print(f"Answer: {result.primary_answer}")
print(f"Confidence: {result.confidence:.2f}")

# Query operational data using natural language
from sutra_core.observability_query import create_observability_interface
obs = create_observability_interface(engine.storage)
obs.query("Show me failed queries in the last hour")
```

### Streaming Responses

```python
import asyncio
from sutra_core.streaming import create_async_engine

async def main():
    engine = ReasoningEngine()
    async_engine = create_async_engine(engine)
    
    # Stream with progressive refinement
    async for chunk in async_engine.ask_stream("What is AI?"):
        print(f"[{chunk.stage}] {chunk.answer} ({chunk.confidence:.2f})")

asyncio.run(main())
```

---

## Architecture Overview

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Interfaces                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Control    â”‚  â”‚   Streamlit  â”‚  â”‚   Direct API         â”‚  â”‚
â”‚  â”‚   Center     â”‚  â”‚   Client     â”‚  â”‚   Integration        â”‚  â”‚
â”‚  â”‚ (React SPA)  â”‚  â”‚  (Python UI) â”‚  â”‚  (REST/Streaming)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API Layer                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Sutra Hybrid API (FastAPI)                  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ REST API   â”‚  â”‚  Streaming   â”‚  â”‚  OpenAI Compatâ”‚   â”‚  â”‚
â”‚  â”‚  â”‚ Endpoints  â”‚  â”‚  SSE (NEW)   â”‚  â”‚  Endpoints    â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Reasoning Engine (Core)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ReasoningEngine (sutra_core)                            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   Query     â”‚  â”‚   Path      â”‚  â”‚    MPPA         â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  Processor  â”‚  â”‚  Finding    â”‚  â”‚  Consensus      â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  NEW: Production Features (Integrated)                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Event      â”‚  â”‚  Quality    â”‚  â”‚   Streaming     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  Emitter    â”‚  â”‚  Gates      â”‚  â”‚   Processor     â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Events as Concepts
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Storage Layer (Rust - Production Grade)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            ConcurrentStorage (Rust)                      â”‚  â”‚
â”‚  â”‚  â€¢ 57K writes/sec, <0.01ms reads                         â”‚  â”‚
â”‚  â”‚  â€¢ Write-Ahead Log (WAL) for durability                  â”‚  â”‚
â”‚  â”‚  â€¢ Native HNSW vector index                              â”‚  â”‚
â”‚  â”‚  â€¢ Lock-free concurrent access                           â”‚  â”‚
â”‚  â”‚  â€¢ Single-file storage.dat                               â”‚  â”‚
â”‚  â”‚  â€¢ Events stored as queryable concepts (NEW)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  TCP Binary Protocol (Port 50051)                              â”‚
â”‚  Grid Events Storage (Port 50052)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Distributed Grid (Optional - Production)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Grid Master    â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚      Grid Agents              â”‚  â”‚
â”‚  â”‚  (Orchestration)â”‚        â”‚  (Node Management)            â”‚  â”‚
â”‚  â”‚  Port 7001/7002 â”‚        â”‚  Port 8001                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Event Flow Architecture (NEW)

```
Application Operations
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EventEmitter (events.py)         â”‚
â”‚   - Query events                   â”‚
â”‚   - Learning events                â”‚
â”‚   - Storage events                 â”‚
â”‚   - Performance events              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ TCP Binary Protocol
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sutra Storage (Rust)             â”‚
â”‚   Events = Concepts + Associations â”‚
â”‚   - Temporal associations          â”‚
â”‚   - Categorical associations       â”‚
â”‚   - Causal associations            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Vector Search + Graph
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ObservabilityQueryInterface      â”‚
â”‚   Natural Language Queries:        â”‚
â”‚   "Show me slow queries today"     â”‚
â”‚   "What causes high latency?"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Production Features

### 1. Self-Observability System âœ¨

**Philosophy:** Eat your own dogfood - Sutra monitors itself using its own reasoning.

**Location:** `sutra_core/events.py`

**Features:**
- Automatic event emission for all operations
- Events stored as concepts in knowledge graph
- 30+ event types tracked
- Zero external dependencies

**Event Types:**
```python
# Query Events
QUERY_RECEIVED
QUERY_COMPLETED  
QUERY_FAILED
QUERY_LOW_CONFIDENCE
QUERY_HIGH_LATENCY

# Learning Events
LEARN_RECEIVED
LEARN_COMPLETED
LEARN_FAILED
LEARN_BATCH_START
LEARN_BATCH_COMPLETED

# Storage Events
STORAGE_READ
STORAGE_WRITE
STORAGE_ERROR
STORAGE_SLOW

# And more...
```

**Usage:**
```python
from sutra_core import ReasoningEngine

# Events automatically enabled
engine = ReasoningEngine()

# Operations emit events automatically
result = engine.ask("What is AI?")  # Emits: QUERY_RECEIVED, QUERY_COMPLETED

# Query events using natural language
from sutra_core.observability_query import create_observability_interface
obs = create_observability_interface(engine.storage)

# Natural language queries
obs.query("Show me failed queries in the last hour")
obs.query("What's the average query latency today?")
obs.query("Which queries have low confidence?")
```

**Documentation:** `docs/PRODUCTION_ENHANCEMENTS.md`

---

### 2. Quality Gates & Confidence Calibration ğŸ›¡ï¸

**Prevents low-quality responses** - knows when to say "I don't know".

**Location:** `sutra_core/quality_gates.py`

**Features:**
- Confidence calibration based on learned patterns
- Quality gate validation (min confidence, consensus, paths)
- Uncertainty quantification with explainable factors
- Automatic "I don't know" for uncertain answers

**Quality Gate Presets:**
```python
from sutra_core.quality_gates import (
    STRICT_QUALITY_GATE,      # High standards (medical/legal)
    MODERATE_QUALITY_GATE,    # Balanced (general chatbot)
    LENIENT_QUALITY_GATE      # Permissive (exploratory)
)
```

**Usage:**
```python
from sutra_core.quality_gates import create_quality_validator, MODERATE_QUALITY_GATE

# Create validator
validator = create_quality_validator(engine.storage, MODERATE_QUALITY_GATE)

# Process query
result = engine.ask("What is quantum computing?")

# Validate quality
assessment = validator.validate(
    raw_confidence=result.confidence,
    consensus_strength=result.consensus_strength,
    num_paths=len(result.supporting_paths),
    has_evidence=bool(result.supporting_paths),
    query_type="what"
)

if assessment.passed:
    print(f"âœ… {result.primary_answer}")
else:
    print(f"âŒ {assessment.recommendation}")
    # Return "I don't know" to user
```

**Configuration:**
```python
# Custom quality gate
CRITICAL_GATE = QualityGate(
    min_confidence=0.8,
    min_consensus=0.9,
    min_paths=3,
    require_evidence=True
)
```

**Documentation:** `docs/PRODUCTION_ENHANCEMENTS.md`

---

### 3. Streaming Responses âš¡

**Progressive answer refinement** - 10x faster perceived performance.

**Location:** `sutra_core/streaming.py`, `sutra_hybrid/api/streaming_endpoints.py`

**Features:**
- Server-Sent Events (SSE) protocol
- 4 stages: initial (60ms), refining (160ms), consensus (360ms), complete
- Progressive MPPA consensus
- Quality gate validation on final chunk
- Event emission integration

**Streaming Stages:**
```
âš¡ Initial (60ms):   First path found
ğŸ”„ Refining (160ms): Accumulating paths 2-4
ğŸ¯ Consensus (360ms): Building multi-path consensus
âœ… Complete (500ms): Final answer with quality check
```

**Python Usage:**
```python
import asyncio
from sutra_core.streaming import create_async_engine

async def main():
    engine = ReasoningEngine()
    async_engine = create_async_engine(engine)
    
    async for chunk in async_engine.ask_stream("What is AI?"):
        print(f"[{chunk.stage}] {chunk.answer} ({chunk.confidence:.2f})")
        if chunk.is_final:
            print("Done!")

asyncio.run(main())
```

**JavaScript Usage:**
```javascript
// Browser/Node.js
import { streamQuery } from './examples/streaming_client';

await streamQuery('What is AI?', {
    onChunk: (chunk) => {
        console.log(`${chunk.stage}: ${chunk.answer}`);
    },
    onComplete: () => console.log('Done!')
});
```

**React Hook:**
```jsx
import { useStreamingQuery } from './examples/streaming_client';

function ChatBot() {
    const { answer, confidence, stage, stream } = useStreamingQuery();
    
    return (
        <div>
            <button onClick={() => stream("What is AI?")}>Ask</button>
            <div>{stage}: {answer} ({confidence}%)</div>
        </div>
    );
}
```

**API Endpoint:**
```
POST http://localhost:8001/sutra/stream/query
Content-Type: application/json

{
    "query": "What is AI?",
    "max_concepts": 10,
    "enable_quality_gates": true
}
```

**Documentation:** `docs/STREAMING.md`

---

### 4. Natural Language Observability Queries ğŸ”

**Query your system's behavior** using natural language.

**Location:** `sutra_core/observability_query.py`

**Features:**
- Time-range extraction (last hour, today, last week)
- Event type filtering (failed, slow, low confidence)
- Metric computation (count, average, max, min)
- Automatic insights generation

**Example Queries:**
```python
from sutra_core.observability_query import create_observability_interface

obs = create_observability_interface(engine.storage)

# Performance monitoring
obs.query("Show me slow queries in the last hour")
obs.query("What's the average response time today?")

# Error analysis
obs.query("Show me all failed queries")
obs.query("What errors occurred today?")

# Confidence analysis  
obs.query("Show me queries with low confidence")
obs.query("What knowledge gaps do we have?")

# System health
obs.query("How many events in the last hour?")
obs.query("Show me system errors")
```

**Response Format:**
```python
{
    'query': 'Show me failed queries today',
    'answer': 'Found 3 failed queries...',
    'events': [...],  # Top 10 events
    'total_events': 3,
    'insights': [
        'Most common error type: No reasoning paths found',
        'âš ï¸ High volume detected'
    ],
    'time_range': {...}
}
```

**Documentation:** `docs/PRODUCTION_ENHANCEMENTS.md`

---

## Deployment

### Production Deployment (Docker Compose)

**Single Command:**
```bash
./sutra-deploy.sh up
```

**What Gets Deployed:**
1. **sutra-storage** (Rust) - Port 50051
2. **sutra-api** (FastAPI) - Port 8000
3. **sutra-hybrid** (FastAPI + NLG + Streaming) - Port 8001
4. **sutra-client** (Streamlit) - Port 8080
5. **sutra-control** (React SPA) - Port 9000
6. **sutra-markdown-web** - Port 8002
7. **sutra-ollama** (Embeddings) - Port 11434
8. **Grid Master** - Ports 7001/7002
9. **Grid Agent** - Port 8001
10. **Grid Events Storage** - Port 50052
11. **Bulk Ingester** (Optional) - Port 8005

**Environment Variables:**
```bash
# Storage
SUTRA_STORAGE_SERVER=storage-server:50051
SUTRA_STORAGE_MODE=server  # or "local"

# API
SUTRA_API_PORT=8000
SUTRA_HYBRID_PORT=8001
SUTRA_CONTROL_PORT=9000

# Embeddings
SUTRA_OLLAMA_URL=http://ollama:11434
OLLAMA_MODEL=granite-embedding:30m

# NLG
SUTRA_NLG_ENABLED=true
SUTRA_NLG_TONE=friendly

# Rate Limits
SUTRA_RATE_LIMIT_LEARN=30
SUTRA_RATE_LIMIT_REASON=60
```

### Kubernetes Deployment

See `docs/DEPLOYMENT.md` for full Kubernetes manifests.

**Quick Deploy:**
```bash
kubectl apply -f k8s/
```

### Manual Deployment

```bash
# 1. Start storage
cd packages/sutra-storage
cargo run --release

# 2. Start hybrid API
cd packages/sutra-hybrid
uvicorn sutra_hybrid.api.app:app --host 0.0.0.0 --port 8001

# 3. Start control center
cd packages/sutra-control
docker build -t sutra-control .
docker run -p 9000:80 sutra-control
```

---

## Configuration

### ReasoningEngine Configuration

```python
from sutra_core import ReasoningEngine
from sutra_core.graph.concepts import AssociationType

engine = ReasoningEngine(
    # Storage
    storage_path="./knowledge",
    use_rust_storage=True,
    
    # Caching
    enable_caching=True,
    max_cache_size=1000,
    cache_ttl_seconds=3600,
    
    # Learning
    enable_central_links=True,
    central_link_confidence=0.6,
    central_link_type=AssociationType.COMPOSITIONAL,
    
    # Performance
    enable_batch_embeddings=True,
    embedding_model="google/embeddinggemma-300m",
    enable_parallel_associations=True,
    association_workers=4,
)
```

### Quality Gate Configuration

```python
from sutra_core.quality_gates import QualityGate

# High-stakes (medical, legal)
CRITICAL_GATE = QualityGate(
    min_confidence=0.8,
    min_consensus=0.9,
    min_paths=3,
    require_evidence=True
)

# General use
MODERATE_GATE = QualityGate(
    min_confidence=0.3,
    min_consensus=0.5,
    min_paths=1,
    require_evidence=True
)

# Exploratory
LENIENT_GATE = QualityGate(
    min_confidence=0.2,
    min_consensus=0.3,
    min_paths=1,
    require_evidence=False
)
```

### Streaming Configuration

```python
from sutra_core.streaming import StreamingQueryProcessor

processor = StreamingQueryProcessor(
    query_processor=engine.query_processor,
    path_finder=engine.path_finder,
    mppa=engine.mppa,
    storage=engine.storage,
    target_paths=5,              # How many paths to find
    min_paths_for_refinement=2,  # When to start refining
)
```

---

## Monitoring & Observability

### Natural Language Queries

**Instead of parsing logs, ask questions:**

```python
from sutra_core.observability_query import create_observability_interface

obs = create_observability_interface(engine.storage)

# Performance
result = obs.query("What's the average latency in the last hour?")
print(result['answer'])
print(result['insights'])

# Errors
errors = obs.query("Show me all failed operations today")
for event in errors['events']:
    print(event['content'])

# Confidence
low_conf = obs.query("How many low confidence queries today?")
if low_conf['total_events'] > 50:
    print("âš ï¸ High volume of uncertain queries")
```

### Metrics via Storage Stats

```python
stats = engine.get_system_stats()

print(f"Concepts: {stats['storage']['total_concepts']}")
print(f"Associations: {stats['storage']['total_associations']}")
print(f"Cache hit rate: {stats['query_processing']['cache_hit_rate']:.2%}")
```

### Event Types Summary

| Category | Event Types | Use Case |
|----------|-------------|----------|
| **Query** | received, completed, failed, low_confidence, high_latency | Query performance tracking |
| **Learning** | received, completed, failed, batch operations | Knowledge ingestion monitoring |
| **Storage** | read, write, errors, slow | Storage performance |
| **Embedding** | generated, failed, slow | Embedding service health |
| **Pathfinding** | started, completed, no_results, failed | Reasoning performance |
| **NLG** | generated, failed | NLG service health |
| **System** | healthy, degraded, error | Overall health |

---

## API Reference

### REST API

**Base URL:** `http://localhost:8001/sutra`

#### POST /sutra/query
Standard query with quality gates.

```bash
curl -X POST http://localhost:8001/sutra/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is AI?",
    "semantic_boost": true,
    "max_paths": 5
  }'
```

**Response:**
```json
{
  "answer": "Artificial intelligence is...",
  "confidence": 0.85,
  "confidence_breakdown": {
    "graph_confidence": 0.82,
    "semantic_confidence": 0.88,
    "final_confidence": 0.85
  },
  "reasoning_paths": [...],
  "explanation": "..."
}
```

#### POST /sutra/stream/query
Streaming query with progressive refinement.

```bash
curl -X POST http://localhost:8001/sutra/stream/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is AI?",
    "max_concepts": 10,
    "enable_quality_gates": true
  }'
```

**Response:** Server-Sent Events stream
```
event: chunk
data: {"stage": "initial", "answer": "...", "confidence": 0.6, ...}

event: chunk
data: {"stage": "refining", "answer": "...", "confidence": 0.75, ...}

event: chunk
data: {"stage": "complete", "answer": "...", "confidence": 0.85, "is_final": true}
```

#### POST /sutra/learn
Learn new knowledge.

```bash
curl -X POST http://localhost:8001/sutra/learn \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Machine learning is a subset of AI",
    "source": "training_data",
    "category": "ai_concepts"
  }'
```

#### GET /sutra/health
Health check with quality metrics.

```bash
curl http://localhost:8001/sutra/health
```

**Response:**
```json
{
  "status": "healthy",
  "version": "2.0.0",
  "uptime_seconds": 3600,
  "total_concepts": 1000,
  "total_associations": 5000
}
```

### Python SDK

```python
from sutra_core import ReasoningEngine

engine = ReasoningEngine()

# Synchronous
result = engine.ask("What is AI?")

# Streaming
from sutra_core.streaming import create_async_engine
async_engine = create_async_engine(engine)

async for chunk in async_engine.ask_stream("What is AI?"):
    print(chunk.answer)
```

---

## Best Practices

### 1. Quality Gates

**Match gate to use case:**
- **High-stakes** (medical/legal): `STRICT_QUALITY_GATE`
- **General chatbot**: `MODERATE_QUALITY_GATE`
- **Exploratory**: `LENIENT_QUALITY_GATE`

**Example:**
```python
from sutra_core.quality_gates import create_quality_validator, STRICT_QUALITY_GATE

# Medical application
validator = create_quality_validator(engine.storage, STRICT_QUALITY_GATE)

result = engine.ask("What is the treatment for X?")
assessment = validator.validate(
    raw_confidence=result.confidence,
    consensus_strength=result.consensus_strength,
    num_paths=len(result.supporting_paths),
    has_evidence=bool(result.supporting_paths)
)

if not assessment.passed:
    # Don't show uncertain medical advice
    return "Please consult a healthcare professional"
```

### 2. Event Monitoring

**Set up alerts:**
```python
# Check for anomalies
obs = create_observability_interface(engine.storage)

low_conf = obs.query("How many low confidence queries in last hour?")
if low_conf['total_events'] > 50:
    send_alert("High uncertainty detected")

failures = obs.query("Show me failed queries in last hour")
if failures['total_events'] > 10:
    send_alert("High failure rate")
```

### 3. Streaming UX

**Show progress indicators:**
```jsx
// React example
function Answer({ chunk }) {
    const stageEmoji = {
        'initial': 'âš¡',
        'refining': 'ğŸ”„',
        'consensus': 'ğŸ¯',
        'complete': 'âœ…'
    };
    
    return (
        <div>
            <span>{stageEmoji[chunk.stage]}</span>
            <span>{chunk.answer}</span>
            <span>{(chunk.confidence * 100).toFixed(0)}%</span>
        </div>
    );
}
```

### 4. Caching Strategy

```python
# Enable caching for read-heavy workloads
engine = ReasoningEngine(
    enable_caching=True,
    max_cache_size=1000,
    cache_ttl_seconds=3600  # 1 hour
)

# Check cache performance
stats = engine.get_system_stats()
print(f"Cache hit rate: {stats['query_processing']['cache_hit_rate']:.2%}")

# Invalidate cache after bulk learning
engine.learn_batch(concepts)
# Cache automatically invalidated
```

### 5. Production Logging

```python
import logging

# Configure structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Events automatically logged + stored in graph
engine = ReasoningEngine()  # Events auto-enabled
```

---

## Troubleshooting

### Events Not Appearing

**Check:**
```python
# 1. Verify event emitter is enabled
print(engine._event_emitter.enabled)  # Should be True

# 2. Check storage connection
print(engine.storage.stats())  # Should show concepts

# 3. Query recent events
obs.query("Show me events in the last minute")
```

**Fix:**
```python
# Re-enable if disabled
engine._event_emitter.enabled = True
```

### Quality Gates Too Strict

**Symptoms:** Too many "I don't know" responses

**Fix:**
```python
# Use lenient gate
from sutra_core.quality_gates import LENIENT_QUALITY_GATE
validator = create_quality_validator(engine.storage, LENIENT_QUALITY_GATE)
```

### Streaming Slow

**Diagnosis:**
```python
obs.query("Show me path finding operations > 500ms")
```

**Fixes:**
1. Reduce target paths: `target_paths=3`
2. Reduce max depth: `max_depth=3`
3. Optimize graph density (remove weak associations)

### High Latency

**Check:**
```python
obs.query("Show me high latency queries today")

# Check specific components
obs.query("Show me slow storage operations")
obs.query("Show me slow embedding generation")
```

**Common causes:**
- Too many concepts (>100K): Consider sharding
- Slow embeddings: Check Ollama service
- Network latency: Check TCP connection

---

## Performance Tuning

### Storage Performance

**Already optimized:**
- 57K writes/sec
- <0.01ms reads
- Lock-free concurrent access

**Tuning:**
```python
# Batch learning for high throughput
concepts = [(content, source, category) for ...]
engine.learn_batch(concepts, batch_size=50)
```

### Query Performance

**Caching:**
```python
engine = ReasoningEngine(
    enable_caching=True,
    max_cache_size=10000,  # Increase for more cache
)
```

**Parallel associations:**
```python
engine = ReasoningEngine(
    enable_parallel_associations=True,
    association_workers=8,  # Match CPU cores
)
```

### Streaming Performance

**Latency targets:**
- Initial response: < 100ms
- Final response: < 500ms

**Tuning:**
```python
processor = StreamingQueryProcessor(
    target_paths=3,              # Fewer paths = faster
    min_paths_for_refinement=1,  # Start refining earlier
)
```

---

## Summary

**Production-Ready Features:**
- âœ… **Self-Observability**: NL queries over operational data
- âœ… **Quality Gates**: Confidence calibration + "I don't know"
- âœ… **Streaming**: 10x faster perceived performance
- âœ… **Event System**: Zero external dependencies

**Performance:**
- Storage: 57K writes/sec, <0.01ms reads
- Streaming: First byte in 60ms
- Quality: Same accuracy, honest uncertainty

**Philosophy:**
- Eat your own dogfood (self-monitoring)
- Zero external dependencies
- 100% explainable
- Production-grade reliability

**Next Steps:**
1. Deploy: `./sutra-deploy.sh up`
2. Query: Try natural language observability
3. Stream: Test progressive refinement
4. Monitor: Set up quality alerts

**Full Documentation:**
- Architecture: `WARP.md`
- Production enhancements: `docs/PRODUCTION_ENHANCEMENTS.md`
- Streaming: `docs/STREAMING.md`
- Deployment: `docs/DEPLOYMENT.md`

---

**Need Help?**
- Query your system: `obs.query("What issues occurred today?")`
- Check logs: `./sutra-deploy.sh logs`
- Review events: Use natural language queries

**Your production AI is ready!** ğŸš€
