# Advanced Association Extraction Strategies

## Problem with Current Approach

**Regex patterns** are:
- âŒ Brittle (don't handle variations)
- âŒ Language-specific (English only)
- âŒ Surface-level (miss semantic relationships)
- âŒ Manual (require human curation)

## Better Approaches (Ranked by Impact)

---

## ðŸ¥‡ Option 1: Dependency Parsing with spaCy (RECOMMENDED)

**What it does**: Extracts grammatical relationships using linguistic rules

### How it Works

```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Dogs are mammals")

for token in doc:
    print(f"{token.text} -> {token.dep_} -> {token.head.text}")

# Output:
# Dogs -> nsubj -> are     (nominal subject)
# are -> ROOT -> are       (main verb)
# mammals -> attr -> are   (attribute)
```

### Implementation

```python
def extract_associations_spacy(content: str) -> List[Association]:
    """Extract associations using dependency parsing."""
    doc = nlp(content)
    associations = []
    
    for token in doc:
        # Pattern 1: Subject-Verb-Object (X verb Y)
        if token.pos_ == "VERB":
            subjects = [child for child in token.children if child.dep_ in ("nsubj", "nsubjpass")]
            objects = [child for child in token.children if child.dep_ in ("dobj", "attr", "pobj")]
            
            for subj in subjects:
                for obj in objects:
                    # Determine association type from verb
                    assoc_type = classify_verb_relation(token.lemma_)
                    associations.append(Association(
                        source=subj.text,
                        target=obj.text,
                        type=assoc_type,
                        confidence=0.8
                    ))
        
        # Pattern 2: Noun compounds (machine learning, deep learning)
        if token.dep_ == "compound":
            associations.append(Association(
                source=token.text,
                target=token.head.text,
                type=AssociationType.COMPOSITIONAL,
                confidence=0.7
            ))
        
        # Pattern 3: Prepositional relationships (dog in house)
        if token.dep_ == "prep":
            pobj = [child for child in token.children if child.dep_ == "pobj"]
            for obj in pobj:
                associations.append(Association(
                    source=token.head.text,
                    target=obj.text,
                    type=AssociationType.SPATIAL,
                    confidence=0.6
                ))
    
    return associations

def classify_verb_relation(verb_lemma: str) -> AssociationType:
    """Map verb to association type."""
    verb_mappings = {
        "cause": AssociationType.CAUSAL,
        "create": AssociationType.CAUSAL,
        "produce": AssociationType.CAUSAL,
        "be": AssociationType.HIERARCHICAL,
        "have": AssociationType.COMPOSITIONAL,
        "contain": AssociationType.COMPOSITIONAL,
        "happen": AssociationType.TEMPORAL,
        "follow": AssociationType.TEMPORAL,
    }
    return verb_mappings.get(verb_lemma, AssociationType.SEMANTIC)
```

### Advantages
- âœ… **Handles all forms**: "is", "are", "was", "were", "being"
- âœ… **Universal patterns**: Works for any grammatical structure
- âœ… **50+ languages**: spaCy supports multilingual
- âœ… **Already integrated**: We use spaCy for embeddings
- âœ… **Fast**: 10-20K tokens/sec on CPU

### Performance Impact
- Speed: ~5-10ms per concept (vs 2ms current)
- Coverage: 80-90% of relationships (vs 30-40% regex)
- Accuracy: 85-90% (vs 95% regex when matched)

### Example Output

```python
Input: "Dogs are mammals that live in houses"

Extracted:
1. dogs -> HIERARCHICAL -> mammals (from "are" verb)
2. mammals -> SPATIAL -> houses (from "in" preposition)
3. live -> mammals (verb relation)
```

---

## ðŸ¥ˆ Option 2: Semantic Role Labeling (SRL)

**What it does**: Identifies "who did what to whom" using deep learning

### How it Works

Uses AllenNLP's SRL model to extract predicate-argument structures:

```python
from allennlp.predictors.predictor import Predictor

predictor = Predictor.from_path(
    "https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.03.24.tar.gz"
)

result = predictor.predict(sentence="Climate change is caused by greenhouse emissions")

# Output:
# {
#   "verbs": [
#     {
#       "verb": "caused",
#       "description": "[ARG1: Climate change] is [V: caused] [ARG0: by greenhouse emissions]",
#       "tags": ["B-ARG1", "I-ARG1", "O", "B-V", "B-ARG0", "I-ARG0", "I-ARG0"]
#     }
#   ]
# }
```

### Implementation

```python
def extract_associations_srl(content: str) -> List[Association]:
    """Extract associations using semantic role labeling."""
    result = srl_predictor.predict(sentence=content)
    associations = []
    
    for verb_info in result["verbs"]:
        verb = verb_info["verb"]
        
        # Extract arguments
        args = parse_srl_tags(verb_info["tags"], content.split())
        
        # ARG0 = agent (who/what does action)
        # ARG1 = patient (who/what receives action)
        # ARG2 = beneficiary, instrument, etc.
        
        if "ARG0" in args and "ARG1" in args:
            assoc_type = classify_verb_to_type(verb)
            associations.append(Association(
                source=args["ARG0"],
                target=args["ARG1"],
                type=assoc_type,
                confidence=0.85
            ))
    
    return associations
```

### Advantages
- âœ… **Deep semantic understanding**: Not just grammar, but meaning
- âœ… **Captures complex relations**: Multi-clause sentences
- âœ… **State-of-the-art accuracy**: 85-90% F1 score
- âœ… **Handles ambiguity**: Better than shallow parsing

### Disadvantages
- âŒ **Slower**: 50-100ms per sentence (GPU helps)
- âŒ **Requires model**: ~500MB download
- âŒ **GPU recommended**: CPU is 5-10x slower

---

## ðŸ¥‰ Option 3: Open Information Extraction (OpenIE)

**What it does**: Extracts (subject, relation, object) triples automatically

### How it Works

Stanford OpenIE or AllenNLP's OpenIE:

```python
from openie import StanfordOpenIE

with StanfordOpenIE() as client:
    triples = client.annotate("Dogs are loyal animals that protect homes")
    
# Output:
# [
#   ("Dogs", "are", "loyal animals"),
#   ("Dogs", "protect", "homes"),
#   ("animals", "are", "loyal")
# ]
```

### Implementation

```python
def extract_associations_openie(content: str) -> List[Association]:
    """Extract associations using Open IE."""
    triples = openie_client.annotate(content)
    associations = []
    
    for subject, relation, obj in triples:
        # Classify relation phrase to association type
        assoc_type = classify_relation_phrase(relation)
        
        associations.append(Association(
            source=subject,
            target=obj,
            type=assoc_type,
            confidence=triple.confidence if hasattr(triple, 'confidence') else 0.7
        ))
    
    return associations

def classify_relation_phrase(relation: str) -> AssociationType:
    """Map relation phrase to association type."""
    relation_lower = relation.lower()
    
    if any(word in relation_lower for word in ["cause", "create", "produce", "lead to"]):
        return AssociationType.CAUSAL
    elif any(word in relation_lower for word in ["is", "are", "was", "were", "be"]):
        return AssociationType.HIERARCHICAL
    elif any(word in relation_lower for word in ["have", "contain", "include", "consist"]):
        return AssociationType.COMPOSITIONAL
    # ... more mappings
    
    return AssociationType.SEMANTIC
```

### Advantages
- âœ… **Triple format**: Perfect for knowledge graphs
- âœ… **Relation phrases preserved**: "is caused by", "results in"
- âœ… **No pre-defined patterns**: Learns from text
- âœ… **High recall**: Extracts many relationships

### Disadvantages
- âŒ **Lower precision**: ~60-70% (more noise)
- âŒ **Requires Java**: Stanford CoreNLP dependency
- âŒ **Slower**: 30-50ms per sentence

---

## ðŸ† Option 4: REBEL (Relation Extraction By End-to-end Language generation)

**What it does**: State-of-the-art neural relation extraction using T5

### How it Works

```python
from transformers import pipeline

# Load REBEL model (415M parameters)
triplet_extractor = pipeline(
    'text2text-generation',
    model='Babelscape/rebel-large',
    tokenizer='Babelscape/rebel-large'
)

text = "Climate change is caused by greenhouse gas emissions from fossil fuels"
extracted = triplet_extractor(text, return_tensors=True, return_text=False)

# Output (after decoding):
# [
#   ("climate change", "caused by", "greenhouse gas emissions"),
#   ("greenhouse gas emissions", "from", "fossil fuels")
# ]
```

### Implementation

```python
def extract_associations_rebel(content: str) -> List[Association]:
    """Extract associations using REBEL model."""
    # Generate triples
    generated = rebel_model(content, max_length=256)
    triples = parse_rebel_output(generated[0]['generated_text'])
    
    associations = []
    for subject, relation, obj in triples:
        # REBEL provides relation labels
        assoc_type = map_rebel_relation(relation)
        
        associations.append(Association(
            source=subject,
            target=obj,
            type=assoc_type,
            confidence=0.9  # REBEL has high precision
        ))
    
    return associations
```

### Advantages
- âœ… **State-of-the-art**: 85%+ F1 on benchmark datasets
- âœ… **Wikidata relations**: 220+ relation types
- âœ… **End-to-end**: No manual rules
- âœ… **Handles complex sentences**: Multi-hop reasoning

### Disadvantages
- âŒ **Large model**: 415M-1.3B parameters
- âŒ **GPU required**: Too slow on CPU
- âŒ **Inference time**: 100-200ms per sentence

---

## ðŸŽ¯ Recommended Hybrid Approach

Combine multiple methods for best results:

```python
class HybridAssociationExtractor:
    """Multi-strategy association extraction."""
    
    def __init__(self):
        self.spacy_nlp = spacy.load("en_core_web_sm")
        self.use_srl = False  # Enable if GPU available
        self.use_rebel = False  # Enable for highest quality
    
    def extract(self, content: str, depth: int = 1) -> List[Association]:
        """Extract associations with fallback strategy."""
        
        # Fast path: spaCy dependency parsing (always)
        associations = self.extract_with_spacy(content)
        
        # Deep path: SRL for difficult concepts (depth=2)
        if depth >= 2 and self.use_srl:
            srl_associations = self.extract_with_srl(content)
            associations.extend(srl_associations)
        
        # Premium path: REBEL for critical content
        if depth >= 3 and self.use_rebel:
            rebel_associations = self.extract_with_rebel(content)
            associations.extend(rebel_associations)
        
        # Deduplicate and merge
        return self.merge_associations(associations)
    
    def extract_with_spacy(self, content: str) -> List[Association]:
        """Fast dependency parsing (5-10ms)."""
        doc = self.spacy_nlp(content)
        associations = []
        
        for token in doc:
            # Subject-verb-object patterns
            if token.pos_ == "VERB" and token.dep_ == "ROOT":
                subjects = [c for c in token.children if c.dep_ in ("nsubj", "nsubjpass")]
                objects = [c for c in token.children if c.dep_ in ("dobj", "attr", "acomp")]
                
                for subj in subjects:
                    for obj in objects:
                        assoc_type = self.classify_verb(token.lemma_)
                        associations.append(Association(
                            source=subj.text,
                            target=obj.text,
                            type=assoc_type,
                            confidence=0.75
                        ))
        
        return associations
    
    def merge_associations(self, associations: List[Association]) -> List[Association]:
        """Deduplicate and strengthen overlapping associations."""
        merged = {}
        
        for assoc in associations:
            key = (assoc.source.lower(), assoc.target.lower())
            
            if key in merged:
                # Strengthen confidence if multiple methods agree
                merged[key].confidence = min(1.0, merged[key].confidence + 0.1)
            else:
                merged[key] = assoc
        
        return list(merged.values())
```

---

## Performance Comparison

| Method | Speed | Coverage | Accuracy | GPU | Model Size |
|--------|-------|----------|----------|-----|------------|
| **Regex (current)** | 2ms | 30-40% | 95%* | No | 0 MB |
| **spaCy Dependency** | 5-10ms | 80-90% | 80% | No | 13 MB |
| **OpenIE** | 30-50ms | 85-95% | 60-70% | No | 500 MB |
| **SRL (AllenNLP)** | 50-100ms | 70-80% | 85-90% | Yes | 450 MB |
| **REBEL** | 100-200ms | 90-95% | 85-90% | Yes | 1.6 GB |
| **Hybrid (Recommended)** | 10-20ms | 85-95% | 85% | No | 13 MB |

*95% accuracy only when patterns match (30-40% coverage)

---

## Implementation Plan

### Phase 1: spaCy Dependency Parsing (2-4 hours)

**Goal**: Replace regex with dependency parsing

```bash
# Already have spaCy, just need to use it
pip install spacy  # Already installed
```

**Changes**:
1. Modify `extract_associations_worker()` to use spaCy
2. Add verb classification logic
3. Keep parallel processing (still CPU-bound)

**Expected**: 
- Coverage: 30% â†’ 85%
- Speed: 2ms â†’ 5-10ms per concept
- Overall throughput: 466 â†’ 100-200 concepts/sec (still 5-10x baseline!)

### Phase 2: Hybrid Fallback (1-2 days)

**Goal**: Use spaCy fast, SRL for depth=2

```python
if depth == 1:
    return extract_with_spacy(content)  # Fast
elif depth >= 2:
    return extract_with_spacy(content) + extract_with_srl(content)  # Deep
```

### Phase 3: REBEL for Premium (Optional, 2-3 days)

**Goal**: Highest quality for critical content

```python
if content_importance == "critical":
    return extract_with_rebel(content)
```

---

## Recommendation: Start with spaCy

**Why spaCy Dependency Parsing:**

1. âœ… **Already integrated**: We use spaCy for embeddings
2. âœ… **No new dependencies**: Zero setup
3. âœ… **Fast enough**: 5-10ms still gives 5-10x speedup
4. âœ… **Huge coverage boost**: 30% â†’ 85%
5. âœ… **Parallelizable**: Still CPU-bound, multiprocessing works
6. âœ… **Production ready**: Mature, stable, well-tested

**Implementation effort**: 2-4 hours to replace regex with dependency parsing

Would you like me to implement the spaCy-based extractor now? It will maintain your 16x speedup while fixing the pattern matching issues! ðŸš€
