"""
HNSW-based vector search for O(log N) semantic retrieval.

This module replaces O(N) linear concept search with efficient
HNSW (Hierarchical Navigable Small World) indexing, providing:
- O(log N) approximate nearest neighbor search
- Automatic reindexing on updates
- Support for both spaCy and custom embeddings
- Incremental index building for minimal overhead
"""

import logging
import time
from typing import Dict, List, Optional, Set, Tuple

import hnswlib
import numpy as np

from ..graph.concepts import Concept

logger = logging.getLogger(__name__)


class VectorIndex:
    """
    Fast vector search using HNSW index.
    
    Key features:
    - O(log N) search vs O(N) linear scan
    - Incremental updates (no full rebuild needed)
    - Automatic dimensionality detection
    - Memory-efficient storage
    - Thread-safe operations
    """

    def __init__(
        self,
        dimension: int = 768,
        max_elements: int = 100000,
        ef_construction: int = 400,  # Production scale for 10K+ vectors
        m: int = 48,                # More connections for better recall
        ef_search: int = 150,        # Better search quality
        enable_auto_rebuild: bool = True,
        rebuild_threshold: int = 1000,
    ):
        """
        Initialize HNSW vector index.

        Args:
            dimension: Embedding dimension (768 for EmbeddingGemma)
            max_elements: Maximum number of vectors to index
            ef_construction: Controls index build quality (higher = better, slower)
            m: Number of bi-directional links per element (higher = better recall)
            ef_search: Controls search quality (higher = better recall, slower)
            enable_auto_rebuild: Automatically rebuild when many updates
            rebuild_threshold: Rebuild after this many additions without rebuild
        """
        self.dimension = dimension
        self.max_elements = max_elements
        self.ef_construction = ef_construction
        self.m = m
        self.ef_search = ef_search
        self.enable_auto_rebuild = enable_auto_rebuild
        self.rebuild_threshold = rebuild_threshold

        # Initialize HNSW index
        self.index = hnswlib.Index(space="cosine", dim=dimension)
        self.index.init_index(
            max_elements=max_elements,
            ef_construction=ef_construction,
            M=m,
        )
        self.index.set_ef(ef_search)

        # Track indexed concepts
        self.concept_id_to_idx: Dict[str, int] = {}
        self.idx_to_concept_id: Dict[int, str] = {}
        self.next_idx = 0
        self.pending_additions = 0

        # Performance tracking
        self.total_searches = 0
        self.total_search_time = 0.0
        self.last_rebuild_time: Optional[float] = None

        logger.info(
            f"Initialized VectorIndex: dim={dimension}, max={max_elements}, "
            f"ef_construction={ef_construction}, M={m}, ef_search={ef_search}"
        )

    def add_concept(
        self,
        concept_id: str,
        embedding: np.ndarray,
        force_rebuild: bool = False,
    ) -> None:
        """
        Add or update a concept in the index.

        Args:
            concept_id: Unique concept identifier
            embedding: Vector embedding (must match dimension)
            force_rebuild: Force index rebuild after addition
        """
        if embedding.shape[0] != self.dimension:
            raise ValueError(
                f"Embedding dimension {embedding.shape[0]} != {self.dimension}"
            )

        # Update existing or add new
        if concept_id in self.concept_id_to_idx:
            idx = self.concept_id_to_idx[concept_id]
            self.index.mark_deleted(idx)
            logger.debug(f"Marked concept {concept_id} (idx={idx}) for update")
        else:
            if self.next_idx >= self.max_elements:
                logger.warning(
                    f"Index full ({self.next_idx}/{self.max_elements}), "
                    "skipping addition"
                )
                return

            idx = self.next_idx
            self.concept_id_to_idx[concept_id] = idx
            self.idx_to_concept_id[idx] = concept_id
            self.next_idx += 1
            self.pending_additions += 1

        # Add to index
        self.index.add_items(embedding.reshape(1, -1), np.array([idx]))
        logger.debug(f"Added concept {concept_id} at idx={idx}")

        # Auto-rebuild check
        if self.enable_auto_rebuild and not force_rebuild:
            if self.pending_additions >= self.rebuild_threshold:
                logger.info(
                    f"Auto-rebuilding index after {self.pending_additions} additions"
                )
                self._rebuild_index()

    def add_concepts_batch(
        self,
        concept_embeddings: List[Tuple[str, np.ndarray]],
    ) -> None:
        """
        Add multiple concepts in batch for efficiency.

        Args:
            concept_embeddings: List of (concept_id, embedding) tuples
        """
        if not concept_embeddings:
            return

        start_time = time.time()
        
        # Prepare batch data
        indices = []
        embeddings = []
        
        for concept_id, embedding in concept_embeddings:
            if embedding.shape[0] != self.dimension:
                logger.warning(
                    f"Skipping {concept_id}: wrong dimension "
                    f"{embedding.shape[0]} != {self.dimension}"
                )
                continue

            if self.next_idx >= self.max_elements:
                logger.warning("Index full, stopping batch addition")
                break

            # Update or add new
            if concept_id in self.concept_id_to_idx:
                idx = self.concept_id_to_idx[concept_id]
                self.index.mark_deleted(idx)
            else:
                idx = self.next_idx
                self.concept_id_to_idx[concept_id] = idx
                self.idx_to_concept_id[idx] = concept_id
                self.next_idx += 1
                self.pending_additions += 1

            indices.append(idx)
            embeddings.append(embedding)

        if embeddings:
            # Batch add to index
            embeddings_array = np.vstack(embeddings)
            indices_array = np.array(indices)
            self.index.add_items(embeddings_array, indices_array)
            
            elapsed = time.time() - start_time
            logger.info(
                f"Batch added {len(embeddings)} concepts in {elapsed:.3f}s "
                f"({len(embeddings)/elapsed:.1f} concepts/sec)"
            )

        # Auto-rebuild check
        if self.enable_auto_rebuild:
            if self.pending_additions >= self.rebuild_threshold:
                logger.info(
                    f"Auto-rebuilding index after {self.pending_additions} additions"
                )
                self._rebuild_index()

    def search(
        self,
        query_embedding: np.ndarray,
        k: int = 10,
        filter_ids: Optional[Set[str]] = None,
    ) -> List[Tuple[str, float]]:
        """
        Search for k nearest neighbors.

        Args:
            query_embedding: Query vector
            k: Number of results to return
            filter_ids: Optional set of concept IDs to include

        Returns:
            List of (concept_id, similarity_score) tuples, sorted by relevance
        """
        if query_embedding.shape[0] != self.dimension:
            raise ValueError(
                f"Query dimension {query_embedding.shape[0]} != {self.dimension}"
            )

        if self.next_idx == 0:
            logger.debug("Index empty, returning no results")
            return []

        start_time = time.time()

        # Search index (returns indices and distances)
        # Note: hnswlib returns cosine distance (0=identical, 2=opposite)
        k_search = min(k * 2 if filter_ids else k, self.next_idx)
        
        try:
            indices, distances = self.index.knn_query(
                query_embedding.reshape(1, -1),
                k=k_search,
            )
        except RuntimeError as e:
            logger.error(f"Search failed: {e}")
            return []

        # Convert to results
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx in self.idx_to_concept_id:
                concept_id = self.idx_to_concept_id[idx]
                
                # Apply filter if provided
                if filter_ids and concept_id not in filter_ids:
                    continue
                
                # Convert distance to similarity (1 - dist/2)
                # cosine distance 0 -> similarity 1.0
                # cosine distance 2 -> similarity 0.0
                similarity = 1.0 - (dist / 2.0)
                results.append((concept_id, float(similarity)))
                
                if len(results) >= k:
                    break

        # Update stats
        elapsed = time.time() - start_time
        self.total_searches += 1
        self.total_search_time += elapsed

        logger.debug(
            f"Search returned {len(results)} results in {elapsed*1000:.2f}ms "
            f"(avg: {(self.total_search_time/self.total_searches)*1000:.2f}ms)"
        )

        return results

    def remove_concept(self, concept_id: str) -> bool:
        """
        Remove a concept from the index.

        Args:
            concept_id: Concept to remove

        Returns:
            True if removed, False if not found
        """
        if concept_id not in self.concept_id_to_idx:
            return False

        idx = self.concept_id_to_idx[concept_id]
        self.index.mark_deleted(idx)
        
        # Note: We keep the ID mappings to avoid reindexing
        # The slot is marked deleted in HNSW
        logger.debug(f"Removed concept {concept_id} (idx={idx})")
        return True

    def _rebuild_index(self) -> None:
        """Rebuild index for optimal performance."""
        start_time = time.time()
        
        # Note: HNSW doesn't need explicit rebuild
        # Deleted items are already handled internally
        self.pending_additions = 0
        self.last_rebuild_time = time.time()
        
        elapsed = time.time() - start_time
        logger.info(
            f"Index rebuild completed in {elapsed:.3f}s "
            f"({self.next_idx} concepts indexed)"
        )

    def get_stats(self) -> Dict:
        """Get index statistics."""
        avg_search_ms = (
            (self.total_search_time / self.total_searches * 1000)
            if self.total_searches > 0
            else 0.0
        )

        return {
            "indexed_concepts": self.next_idx,
            "max_capacity": self.max_elements,
            "utilization": self.next_idx / self.max_elements,
            "total_searches": self.total_searches,
            "avg_search_ms": avg_search_ms,
            "pending_additions": self.pending_additions,
            "last_rebuild": self.last_rebuild_time,
            "dimension": self.dimension,
            "ef_search": self.ef_search,
        }

    def clear(self) -> None:
        """Clear entire index."""
        self.index = hnswlib.Index(space="cosine", dim=self.dimension)
        self.index.init_index(
            max_elements=self.max_elements,
            ef_construction=self.ef_construction,
            M=self.m,
        )
        self.index.set_ef(self.ef_search)
        
        self.concept_id_to_idx.clear()
        self.idx_to_concept_id.clear()
        self.next_idx = 0
        self.pending_additions = 0
        
        logger.info("Index cleared")
