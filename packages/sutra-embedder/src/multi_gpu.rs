/// Multi-GPU Distributed Inference
/// 
/// Production-grade multi-GPU support for ultra-high throughput embedding generation.
/// 
/// Key features:
/// - Automatic GPU detection and pooling
/// - Load balancing across GPUs
/// - Parallel batch processing
/// - Fault tolerance and GPU health monitoring
/// - Dynamic GPU allocation
/// 
/// Performance targets:
/// - 1000+ embeddings/sec on single GPU
/// - 5000+ embeddings/sec on 4-GPU setup
/// - 20000+ embeddings/sec on 8-GPU cluster
use anyhow::{anyhow, Result};
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, RwLock, Semaphore};
use tracing::{debug, info, warn, error};

/// Multi-GPU configuration
#[derive(Debug, Clone)]
pub struct MultiGPUConfig {
    /// GPU device IDs to use (empty = auto-detect all)
    pub device_ids: Vec<usize>,
    /// Load balancing strategy
    pub load_balancing: LoadBalancingStrategy,
    /// Maximum concurrent requests per GPU
    pub max_concurrent_per_gpu: usize,
    /// Enable GPU health monitoring
    pub enable_health_checks: bool,
    /// Health check interval (seconds)
    pub health_check_interval_secs: u64,
    /// Retry failed GPU operations
    pub retry_on_failure: bool,
    /// Maximum retry attempts
    pub max_retries: usize,
}

impl Default for MultiGPUConfig {
    fn default() -> Self {
        Self {
            device_ids: Vec::new(), // Auto-detect
            load_balancing: LoadBalancingStrategy::RoundRobin,
            max_concurrent_per_gpu: 4,
            enable_health_checks: true,
            health_check_interval_secs: 30,
            retry_on_failure: true,
            max_retries: 3,
        }
    }
}

/// Load balancing strategies
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum LoadBalancingStrategy {
    /// Round-robin across GPUs
    RoundRobin,
    /// Choose GPU with lowest queue depth
    LeastLoaded,
    /// Choose GPU with best performance history
    PerformanceBased,
    /// Random selection
    Random,
}

/// GPU worker pool for distributed inference
pub struct MultiGPUPool {
    config: MultiGPUConfig,
    workers: Arc<RwLock<Vec<GPUWorker>>>,
    request_tx: mpsc::UnboundedSender<DistributedRequest>,
    next_worker_idx: Arc<Mutex<usize>>,
    stats: Arc<RwLock<MultiGPUStats>>,
}

/// Individual GPU worker
struct GPUWorker {
    device_id: usize,
    is_healthy: bool,
    active_requests: Arc<Mutex<usize>>,
    total_processed: Arc<Mutex<usize>>,
    total_errors: Arc<Mutex<usize>>,
    average_latency_ms: Arc<Mutex<f32>>,
    embedder: Arc<Mutex<crate::embedder::Embedder>>,
    semaphore: Arc<Semaphore>,
}

/// Distributed inference request
struct DistributedRequest {
    texts: Vec<String>,
    response_tx: tokio::sync::oneshot::Sender<Result<Vec<Vec<f32>>>>,
    device_id: Option<usize>, // Optional: pin to specific GPU
}

/// Multi-GPU performance statistics
#[derive(Debug, Clone, Default)]
pub struct MultiGPUStats {
    pub total_requests: usize,
    pub total_embeddings: usize,
    pub total_errors: usize,
    pub average_latency_ms: f32,
    pub throughput_per_sec: f32,
    pub gpu_utilization: Vec<GPUUtilization>,
}

/// Per-GPU utilization stats
#[derive(Debug, Clone)]
pub struct GPUUtilization {
    pub device_id: usize,
    pub is_healthy: bool,
    pub active_requests: usize,
    pub total_processed: usize,
    pub total_errors: usize,
    pub average_latency_ms: f32,
    pub utilization_percent: f32,
}

impl MultiGPUPool {
    /// Create new multi-GPU pool
    pub async fn new(config: MultiGPUConfig, embedder_config: crate::embedder::EmbedderConfig) -> Result<Self> {
        info!("Initializing multi-GPU pool...");
        
        // Detect available GPUs
        let gpu_devices = Self::detect_gpus(&config)?;
        
        if gpu_devices.is_empty() {
            return Err(anyhow!("No GPUs detected for multi-GPU inference"));
        }
        
        info!("Detected {} GPU(s): {:?}", gpu_devices.len(), gpu_devices);
        
        // Create workers for each GPU
        let mut workers = Vec::new();
        for &device_id in &gpu_devices {
            let worker = Self::create_worker(device_id, &config, embedder_config.clone()).await?;
            workers.push(worker);
        }
        
        let (request_tx, request_rx) = mpsc::unbounded_channel();
        
        let pool = Self {
            config: config.clone(),
            workers: Arc::new(RwLock::new(workers)),
            request_tx,
            next_worker_idx: Arc::new(Mutex::new(0)),
            stats: Arc::new(RwLock::new(MultiGPUStats::default())),
        };
        
        // Spawn request dispatcher
        let pool_clone = pool.clone_for_dispatcher();
        tokio::spawn(async move {
            Self::request_dispatcher(pool_clone, request_rx).await;
        });
        
        // Spawn health monitor if enabled
        if config.enable_health_checks {
            let pool_clone = pool.clone_for_health_monitor();
            tokio::spawn(async move {
                Self::health_monitor(pool_clone).await;
            });
        }
        
        info!("Multi-GPU pool initialized with {} workers", gpu_devices.len());
        
        Ok(pool)
    }
    
    /// Detect available GPUs
    fn detect_gpus(config: &MultiGPUConfig) -> Result<Vec<usize>> {
        if !config.device_ids.is_empty() {
            // Use specified device IDs
            return Ok(config.device_ids.clone());
        }
        
        // Auto-detect GPUs
        let mut devices = Vec::new();
        
        // Check CUDA GPUs
        if let Ok(count) = Self::get_cuda_device_count() {
            for i in 0..count {
                devices.push(i);
            }
        }
        
        // Check ROCm GPUs (Linux only)
        #[cfg(target_os = "linux")]
        {
            if devices.is_empty() {
                if let Ok(count) = Self::get_rocm_device_count() {
                    for i in 0..count {
                        devices.push(i);
                    }
                }
            }
        }
        
        // Metal (macOS) - treat as single device
        #[cfg(target_os = "macos")]
        {
            if devices.is_empty() && Self::has_metal() {
                devices.push(0);
            }
        }
        
        Ok(devices)
    }
    
    /// Get CUDA device count
    fn get_cuda_device_count() -> Result<usize> {
        use std::process::Command;
        
        let output = Command::new("nvidia-smi")
            .args(["--query-gpu=count", "--format=csv,noheader"])
            .output()?;
        
        if !output.status.success() {
            return Ok(0);
        }
        
        let count_str = String::from_utf8_lossy(&output.stdout);
        let count = count_str.lines().count();
        
        Ok(count)
    }
    
    /// Get ROCm device count
    #[cfg(target_os = "linux")]
    fn get_rocm_device_count() -> Result<usize> {
        use std::process::Command;
        
        let output = Command::new("rocm-smi")
            .arg("--showid")
            .output()?;
        
        if !output.status.success() {
            return Ok(0);
        }
        
        let output_str = String::from_utf8_lossy(&output.stdout);
        let count = output_str.lines().filter(|line| line.contains("GPU")).count();
        
        Ok(count)
    }
    
    /// Check if Metal is available (macOS)
    #[cfg(target_os = "macos")]
    fn has_metal() -> bool {
        crate::hardware::HardwareProfile::detect().has_gpu()
    }
    
    /// Create GPU worker
    async fn create_worker(
        device_id: usize,
        config: &MultiGPUConfig,
        embedder_config: crate::embedder::EmbedderConfig,
    ) -> Result<GPUWorker> {
        info!("Creating worker for GPU {}", device_id);
        
        // Configure embedder for specific GPU
        // Note: ONNX Runtime device selection would be set here via environment variables
        // or execution provider options
        
        std::env::set_var("CUDA_VISIBLE_DEVICES", device_id.to_string());
        
        let embedder = crate::embedder::Embedder::new(embedder_config.clone())?;
        
        Ok(GPUWorker {
            device_id,
            is_healthy: true,
            active_requests: Arc::new(Mutex::new(0)),
            total_processed: Arc::new(Mutex::new(0)),
            total_errors: Arc::new(Mutex::new(0)),
            average_latency_ms: Arc::new(Mutex::new(0.0)),
            embedder: Arc::new(Mutex::new(embedder)),
            semaphore: Arc::new(Semaphore::new(config.max_concurrent_per_gpu)),
        })
    }
    
    /// Request dispatcher (load balancer)
    async fn request_dispatcher(
        pool: Arc<RwLock<MultiGPUPoolInternal>>,
        mut request_rx: mpsc::UnboundedReceiver<DistributedRequest>,
    ) {
        info!("Request dispatcher started");
        
        while let Some(request) = request_rx.recv().await {
            let pool = pool.read().await;
            
            // Select worker based on load balancing strategy
            let worker_idx = pool.select_worker(request.device_id).await;
            
            // Spawn task to process request
            let workers = pool.workers.clone();
            tokio::spawn(async move {
                let workers = workers.read().await;
                if let Some(worker) = workers.get(worker_idx) {
                    let result = worker.process_request(request.texts).await;
                    let _ = request.response_tx.send(result);
                }
            });
        }
        
        info!("Request dispatcher stopped");
    }
    
    /// Health monitor (periodic GPU health checks)
    async fn health_monitor(pool: Arc<RwLock<MultiGPUPoolInternal>>) {
        info!("Health monitor started");
        
        let pool_read = pool.read().await;
        let interval_secs = pool_read.config.health_check_interval_secs;
        drop(pool_read);
        
        let mut interval = tokio::time::interval(tokio::time::Duration::from_secs(interval_secs));
        
        loop {
            interval.tick().await;
            
            let pool = pool.read().await;
            let workers = pool.workers.read().await;
            
            for worker in workers.iter() {
                // Check worker health
                if !worker.is_healthy {
                    warn!("GPU {} is unhealthy", worker.device_id);
                    // Attempt recovery
                    // TODO: Implement GPU recovery logic
                }
            }
        }
    }
    
    /// Clone pool for dispatcher
    fn clone_for_dispatcher(&self) -> Arc<RwLock<MultiGPUPoolInternal>> {
        Arc::new(RwLock::new(MultiGPUPoolInternal {
            config: self.config.clone(),
            workers: self.workers.clone(),
            next_worker_idx: self.next_worker_idx.clone(),
            stats: self.stats.clone(),
        }))
    }
    
    /// Clone pool for health monitor
    fn clone_for_health_monitor(&self) -> Arc<RwLock<MultiGPUPoolInternal>> {
        Arc::new(RwLock::new(MultiGPUPoolInternal {
            config: self.config.clone(),
            workers: self.workers.clone(),
            next_worker_idx: self.next_worker_idx.clone(),
            stats: self.stats.clone(),
        }))
    }
    
    /// Submit batch for distributed inference
    pub async fn embed_batch_distributed(&self, texts: Vec<String>) -> Result<Vec<Vec<f32>>> {
        let (response_tx, response_rx) = tokio::sync::oneshot::channel();
        
        self.request_tx.send(DistributedRequest {
            texts,
            response_tx,
            device_id: None,
        }).map_err(|e| anyhow!("Failed to submit request: {}", e))?;
        
        response_rx.await
            .map_err(|e| anyhow!("Failed to receive response: {}", e))?
    }
    
    /// Get current statistics
    pub async fn get_stats(&self) -> MultiGPUStats {
        self.stats.read().await.clone()
    }
}

/// Internal pool state for dispatcher and health monitor
struct MultiGPUPoolInternal {
    config: MultiGPUConfig,
    workers: Arc<RwLock<Vec<GPUWorker>>>,
    next_worker_idx: Arc<Mutex<usize>>,
    #[allow(dead_code)]
    stats: Arc<RwLock<MultiGPUStats>>,
}

impl MultiGPUPoolInternal {
    /// Select worker based on load balancing strategy
    async fn select_worker(&self, device_id: Option<usize>) -> usize {
        // Pin to specific device if requested
        if let Some(id) = device_id {
            return id;
        }
        
        match self.config.load_balancing {
            LoadBalancingStrategy::RoundRobin => {
                let mut idx = self.next_worker_idx.lock().await;
                let workers = self.workers.read().await;
                let selected = *idx % workers.len();
                *idx = (*idx + 1) % workers.len();
                selected
            }
            LoadBalancingStrategy::LeastLoaded => {
                self.select_least_loaded_worker().await
            }
            LoadBalancingStrategy::PerformanceBased => {
                self.select_best_performing_worker().await
            }
            LoadBalancingStrategy::Random => {
                use rand::Rng;
                let workers = self.workers.read().await;
                rand::thread_rng().gen_range(0..workers.len())
            }
        }
    }
    
    /// Select worker with lowest queue depth
    async fn select_least_loaded_worker(&self) -> usize {
        let workers = self.workers.read().await;
        
        let mut min_load = usize::MAX;
        let mut selected = 0;
        
        for (idx, worker) in workers.iter().enumerate() {
            if !worker.is_healthy {
                continue;
            }
            
            let load = *worker.active_requests.lock().await;
            if load < min_load {
                min_load = load;
                selected = idx;
            }
        }
        
        selected
    }
    
    /// Select worker with best performance history
    async fn select_best_performing_worker(&self) -> usize {
        let workers = self.workers.read().await;
        
        let mut best_latency = f32::MAX;
        let mut selected = 0;
        
        for (idx, worker) in workers.iter().enumerate() {
            if !worker.is_healthy {
                continue;
            }
            
            let latency = *worker.average_latency_ms.lock().await;
            if latency < best_latency && latency > 0.0 {
                best_latency = latency;
                selected = idx;
            }
        }
        
        selected
    }
}

impl GPUWorker {
    /// Process request on this GPU
    async fn process_request(&self, texts: Vec<String>) -> Result<Vec<Vec<f32>>> {
        // Acquire semaphore (concurrency limit)
        let _permit = self.semaphore.acquire().await
            .map_err(|e| anyhow!("Failed to acquire semaphore: {}", e))?;
        
        // Track active requests
        {
            let mut active = self.active_requests.lock().await;
            *active += 1;
        }
        
        let start = std::time::Instant::now();
        
        // Process with embedder
        let result = {
            let mut embedder = self.embedder.lock().await;
            embedder.embed_batch(&texts)
        };
        
        let elapsed = start.elapsed().as_millis() as f32;
        
        // Update statistics
        {
            let mut active = self.active_requests.lock().await;
            *active -= 1;
        }
        
        match &result {
            Ok(_) => {
                let mut processed = self.total_processed.lock().await;
                *processed += texts.len();
                
                // Update average latency (exponential moving average)
                let mut avg_latency = self.average_latency_ms.lock().await;
                *avg_latency = if *avg_latency == 0.0 {
                    elapsed
                } else {
                    *avg_latency * 0.9 + elapsed * 0.1
                };
                
                debug!("GPU {} processed {} texts in {:.2}ms", self.device_id, texts.len(), elapsed);
            }
            Err(e) => {
                let mut errors = self.total_errors.lock().await;
                *errors += 1;
                error!("GPU {} error: {}", self.device_id, e);
            }
        }
        
        result
    }
}

impl std::fmt::Display for MultiGPUStats {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "Multi-GPU Statistics:\n  Total Requests: {}\n  Total Embeddings: {}\n  Total Errors: {}\n  Avg Latency: {:.2}ms\n  Throughput: {:.1} emb/sec\n  GPUs: {}",
            self.total_requests,
            self.total_embeddings,
            self.total_errors,
            self.average_latency_ms,
            self.throughput_per_sec,
            self.gpu_utilization.len()
        )?;
        
        for gpu in &self.gpu_utilization {
            write!(
                f,
                "\n    GPU {}: {} requests, {:.2}ms avg, {:.1}% util",
                gpu.device_id,
                gpu.total_processed,
                gpu.average_latency_ms,
                gpu.utilization_percent
            )?;
        }
        
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_multi_gpu_config() {
        let config = MultiGPUConfig::default();
        assert_eq!(config.load_balancing, LoadBalancingStrategy::RoundRobin);
        assert_eq!(config.max_concurrent_per_gpu, 4);
    }
    
    #[tokio::test]
    async fn test_gpu_detection() {
        let config = MultiGPUConfig::default();
        let devices = MultiGPUPool::detect_gpus(&config);
        
        // Should succeed even if no GPUs (returns empty vec)
        assert!(devices.is_ok());
    }
}
